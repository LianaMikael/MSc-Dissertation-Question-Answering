{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_SQuaD_large_new.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUBP35oCDmbF",
        "colab_type": "text"
      },
      "source": [
        "**BERT LARGE on SQuAD 1.1**\n",
        "\n",
        "Based on the code provided at https://github.com/google-research/bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "191zq3ZErihP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime' \n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  print(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wzwke0sxS6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "  \n",
        "\n",
        "!test -d sq_repo || git clone https://github.com/rajpurkar/SQuAD-explorer sq_repo\n",
        "if not 'sq_repo' in sys.path:\n",
        "  sys.path += ['sq_repo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j41oVP0Rdmhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SQuaD 1.1\n",
        "TRAIN_FILE = \"sq_repo/dataset/train-v1.1.json\"\n",
        "EVAL_FILE = \"sq_repo/dataset/dev-v1.1.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYkaAlJNfhul",
        "colab_type": "code",
        "outputId": "39244f22-e05c-48d3-a7f8-7aff38b2adfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "TASK = \"SQuAD_1.1_large_new_1\"\n",
        "#BERT_MODEL = 'wwm_uncased_L-24_H-1024_A-16' #@param {type:\"string\"}\n",
        "BERT_MODEL = 'uncased_L-24_H-1024_A-16' #@param {type:\"string\"}\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "#BERT_PRETRAINED_DIR = 'gs://bert_bucket_new/bert/Whole Word Masking'\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR\n",
        "#!gsutil ls $'gs://bert_bucket_new/bert/Whole Word Masking'\n",
        "\n",
        "BUCKET = 'bert_bucket_new' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/bert/models/{}'.format(BUCKET, TASK)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16 *****\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_config.json\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_model.ckpt.index\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/bert_model.ckpt.meta\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/checkpoint\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-24_H-1024_A-16/vocab.txt\n",
            "***** Model output directory: gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgOsNtxlYurB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import modeling\n",
        "import optimization\n",
        "import tokenization\n",
        "import six\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "WITH_NEGATIVE = False\n",
        "NULL_SCORE_DIFF_THRESHOLD = 0.0\n",
        "TRAIN_BATCH_SIZE = 16#24\n",
        "EVAL_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 3e-5#3e-5\n",
        "NUM_TRAIN_EPOCHS = 2.0\n",
        "WARMUP_PROPORTION = 0.1\n",
        "MAX_SEQ_LENGTH = 386#389#386#384\n",
        "\n",
        "# Model configs\n",
        "VERBOSE_LOGGING = False\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "ITERATIONS_PER_LOOP = 500\n",
        "NUM_TPU_CORES = 8\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "DOC_STRIDE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsdDPPnCW1Hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SquadExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\n",
        "\n",
        "     For examples without an answer, the start and end position are -1.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               qas_id,\n",
        "               question_text,\n",
        "               doc_tokens,\n",
        "               orig_answer_text=None,\n",
        "               char_to_word_offset=None,\n",
        "               start_position=None,\n",
        "               end_position=None,\n",
        "               is_impossible=False):\n",
        "    self.qas_id = qas_id\n",
        "    self.question_text = question_text\n",
        "    self.doc_tokens = doc_tokens\n",
        "    self.orig_answer_text = orig_answer_text\n",
        "    self.start_position = start_position\n",
        "    self.end_position = end_position\n",
        "    self.is_impossible = is_impossible\n",
        "    self.char_to_word_offset = char_to_word_offset\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.__repr__()\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = \"\"\n",
        "    s += \"qas_id: %s\" % (tokenization.printable_text(self.qas_id))\n",
        "    s += \", question_text: %s\" % (\n",
        "        tokenization.printable_text(self.question_text))\n",
        "    s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
        "    if self.start_position:\n",
        "      s += \", start_position: %d\" % (self.start_position)\n",
        "    if self.start_position:\n",
        "      s += \", end_position: %d\" % (self.end_position)\n",
        "    if self.start_position:\n",
        "      s += \", is_impossible: %r\" % (self.is_impossible)\n",
        "    return s\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               unique_id,\n",
        "               example_index,\n",
        "               doc_span_index,\n",
        "               tokens,\n",
        "               token_to_orig_map,\n",
        "               token_is_max_context,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               start_position=None,\n",
        "               end_position=None,\n",
        "               is_impossible=None):\n",
        "    self.unique_id = unique_id\n",
        "    self.example_index = example_index\n",
        "    self.doc_span_index = doc_span_index\n",
        "    self.tokens = tokens\n",
        "    self.token_to_orig_map = token_to_orig_map\n",
        "    self.token_is_max_context = token_is_max_context\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.start_position = start_position\n",
        "    self.end_position = end_position\n",
        "    self.is_impossible = is_impossible\n",
        "\n",
        "\n",
        "def read_squad_examples(input_file, is_training):\n",
        "  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
        "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
        "    input_data = json.load(reader)[\"data\"] ##########\n",
        "\n",
        "  def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  examples = []\n",
        "  for entry in input_data:\n",
        "    for paragraph in entry[\"paragraphs\"]:\n",
        "      paragraph_text = paragraph[\"context\"]\n",
        "      doc_tokens = []\n",
        "      char_to_word_offset = []\n",
        "      prev_is_whitespace = True\n",
        "      for c in paragraph_text:\n",
        "        if is_whitespace(c):\n",
        "          prev_is_whitespace = True\n",
        "        else:\n",
        "          if prev_is_whitespace:\n",
        "            doc_tokens.append(c)\n",
        "          else:\n",
        "            doc_tokens[-1] += c\n",
        "          prev_is_whitespace = False\n",
        "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "      for qa in paragraph[\"qas\"]:\n",
        "        qas_id = qa[\"id\"]\n",
        "        question_text = qa[\"question\"]\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "        orig_answer_text = None\n",
        "        is_impossible = False\n",
        "        if is_training:\n",
        "\n",
        "          if WITH_NEGATIVE:\n",
        "            is_impossible = qa[\"is_impossible\"]\n",
        "          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
        "            raise ValueError(\n",
        "                \"For training, each question should have exactly 1 answer.\")\n",
        "          if not is_impossible:\n",
        "            answer = qa[\"answers\"][0]\n",
        "            orig_answer_text = answer[\"text\"]\n",
        "            answer_offset = answer[\"answer_start\"]\n",
        "            answer_length = len(orig_answer_text)\n",
        "            start_position = char_to_word_offset[answer_offset]\n",
        "            end_position = char_to_word_offset[answer_offset + answer_length -\n",
        "                                               1]\n",
        "            # Only add answers where the text can be exactly recovered from the\n",
        "            # document. If this CAN'T happen it's likely due to weird Unicode\n",
        "            # stuff so we will just skip the example.\n",
        "            #\n",
        "            # Note that this means for training mode, every example is NOT\n",
        "            # guaranteed to be preserved.\n",
        "            \n",
        "            actual_text = \" \".join(\n",
        "                doc_tokens[start_position:(end_position + 1)])\n",
        "            cleaned_answer_text = \" \".join(\n",
        "                tokenization.whitespace_tokenize(orig_answer_text))\n",
        "            if actual_text.find(cleaned_answer_text) == -1:\n",
        "              tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                 actual_text, cleaned_answer_text)\n",
        "              continue\n",
        "          else:\n",
        "            start_position = -1\n",
        "            end_position = -1\n",
        "            orig_answer_text = \"\"\n",
        "\n",
        "        example = SquadExample(\n",
        "            qas_id=qas_id,\n",
        "            question_text=question_text,\n",
        "            doc_tokens=doc_tokens,\n",
        "            orig_answer_text=orig_answer_text,\n",
        "            char_to_word_offset=char_to_word_offset,\n",
        "            start_position=start_position,\n",
        "            end_position=end_position,\n",
        "            is_impossible=is_impossible)\n",
        "        examples.append(example)\n",
        "\n",
        "  return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length, is_training,\n",
        "                                 output_fn):\n",
        "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "  all_tokens = []\n",
        "  unique_id = 1000000000\n",
        "\n",
        "  all_features_matrix = []\n",
        "  for i in range(2):\n",
        "  \n",
        "    all_features_list = []\n",
        "    for (example_index, example) in enumerate(examples[i]):\n",
        "      query_tokens = tokenizer.tokenize(example.question_text)\n",
        "\n",
        "      if len(query_tokens) > max_query_length:\n",
        "        query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "      tok_to_orig_index = []\n",
        "      orig_to_tok_index = []\n",
        "      all_doc_tokens = []\n",
        "      for (i, token) in enumerate(example.doc_tokens):\n",
        "        orig_to_tok_index.append(len(all_doc_tokens))\n",
        "        sub_tokens = tokenizer.tokenize(token)\n",
        "        for sub_token in sub_tokens:\n",
        "          tok_to_orig_index.append(i)\n",
        "          all_doc_tokens.append(sub_token)\n",
        "\n",
        "      tok_start_position = None\n",
        "      tok_end_position = None\n",
        "      if is_training and example.is_impossible:\n",
        "        tok_start_position = -1\n",
        "        tok_end_position = -1\n",
        "      if is_training and not example.is_impossible:\n",
        "        tok_start_position = orig_to_tok_index[example.start_position]\n",
        "        if example.end_position < len(example.doc_tokens) - 1:\n",
        "          tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
        "        else:\n",
        "          tok_end_position = len(all_doc_tokens) - 1\n",
        "        (tok_start_position, tok_end_position) = _improve_answer_span(\n",
        "            all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
        "            example.orig_answer_text)\n",
        "\n",
        "      # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "      max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "      # We can have documents that are longer than the maximum sequence length.\n",
        "      # To deal with this we do a sliding window approach, where we take chunks\n",
        "      # of the up to our max length with a stride of `doc_stride`.\n",
        "      _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "          \"DocSpan\", [\"start\", \"length\"])\n",
        "      doc_spans = []\n",
        "      start_offset = 0\n",
        "      while start_offset < len(all_doc_tokens):\n",
        "        length = len(all_doc_tokens) - start_offset\n",
        "        if length > max_tokens_for_doc:\n",
        "          length = max_tokens_for_doc\n",
        "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "        if start_offset + length == len(all_doc_tokens):\n",
        "          break\n",
        "        start_offset += min(length, doc_stride)\n",
        "\n",
        "      for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "        tokens = []\n",
        "        token_to_orig_map = {}\n",
        "        token_is_max_context = {}\n",
        "        segment_ids = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        segment_ids.append(0)\n",
        "        for token in query_tokens:\n",
        "          tokens.append(token)\n",
        "          segment_ids.append(0)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(0)\n",
        "\n",
        "        for i in range(doc_span.length):\n",
        "          split_token_index = doc_span.start + i\n",
        "          token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "          is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                                 split_token_index)\n",
        "          token_is_max_context[len(tokens)] = is_max_context\n",
        "          tokens.append(all_doc_tokens[split_token_index])\n",
        "          segment_ids.append(1)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        segment_ids.append(1)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "          input_ids.append(0)\n",
        "          input_mask.append(0)\n",
        "          segment_ids.append(0)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length \n",
        "\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "        if is_training and not example.is_impossible:\n",
        "          # For training, if our document chunk does not contain an annotation\n",
        "          # we throw it out, since there is nothing to predict.\n",
        "          doc_start = doc_span.start\n",
        "          doc_end = doc_span.start + doc_span.length - 1\n",
        "          out_of_span = False\n",
        "          if not (tok_start_position >= doc_start and\n",
        "                  tok_end_position <= doc_end):\n",
        "            out_of_span = True\n",
        "          if out_of_span:\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "          else:\n",
        "            doc_offset = len(query_tokens) + 2\n",
        "            start_position = tok_start_position - doc_start + doc_offset\n",
        "            end_position = tok_end_position - doc_start + doc_offset\n",
        "\n",
        "        if is_training and example.is_impossible:\n",
        "          start_position = 0\n",
        "          end_position = 0\n",
        "\n",
        "        if example_index < 20:\n",
        "          tf.logging.info(\"*** Example ***\")\n",
        "          tf.logging.info(\"unique_id: %s\" % (unique_id))\n",
        "          tf.logging.info(\"example_index: %s\" % (example_index))\n",
        "          tf.logging.info(\"doc_span_index: %s\" % (doc_span_index))\n",
        "          tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "              [tokenization.printable_text(x) for x in tokens]))\n",
        "          tf.logging.info(\"token_to_orig_map: %s\" % \" \".join(\n",
        "              [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
        "          tf.logging.info(\"token_is_max_context: %s\" % \" \".join([\n",
        "              \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
        "          ]))\n",
        "          tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "          tf.logging.info(\n",
        "              \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "          tf.logging.info(\n",
        "              \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "          if is_training and example.is_impossible:\n",
        "            tf.logging.info(\"impossible example\")\n",
        "          if is_training and not example.is_impossible:\n",
        "            answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
        "            tf.logging.info(\"start_position: %d\" % (start_position))\n",
        "            tf.logging.info(\"end_position: %d\" % (end_position))\n",
        "            tf.logging.info(\n",
        "                \"answer: %s\" % (tokenization.printable_text(answer_text)))\n",
        "\n",
        "        feature = InputFeatures(\n",
        "            unique_id=unique_id,\n",
        "            example_index=example_index,\n",
        "            doc_span_index=doc_span_index,\n",
        "            tokens=tokens,\n",
        "            token_to_orig_map=token_to_orig_map,\n",
        "            token_is_max_context=token_is_max_context,\n",
        "            input_ids=input_ids,\n",
        "            input_mask=input_mask,\n",
        "            segment_ids=segment_ids,\n",
        "            start_position=start_position,\n",
        "            end_position=end_position,\n",
        "            is_impossible=example.is_impossible)\n",
        "\n",
        "        # Run callback\n",
        "        all_features_list.append(feature)\n",
        "        #output_fn(feature)     \n",
        "      unique_id += 1\n",
        "      all_tokens.append(tokens)\n",
        "      \n",
        "    all_features_matrix.append(all_features_list)\n",
        "  all_features_matrix = np.array(all_features_matrix)\n",
        "  \n",
        "  print(len(all_features_matrix[0]))\n",
        "  print(len(all_features_matrix[1]))\n",
        "  \n",
        "  assert len(all_features_matrix[0]) == len(all_features_matrix[1])\n",
        "  \n",
        "  for i in range(len(all_features_matrix[1])):\n",
        "    f = [all_features_matrix[0][i], all_features_matrix[1][i]]#, all_features_matrix[2][i]]\n",
        "    output_fn(f)\n",
        "    #print(f)\n",
        "\n",
        "  print(\"Processed All Features!!!!!\")\n",
        "  return all_tokens, all_features_matrix\n",
        "      \n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "  # Because of the sliding window approach taken to scoring documents, a single\n",
        "  # token can appear in multiple documents. E.g.\n",
        "  #  Doc: the man went to the store and bought a gallon of milk\n",
        "  #  Span A: the man went to the\n",
        "  #  Span B: to the store and bought\n",
        "  #  Span C: and bought a gallon of\n",
        "  #  ...\n",
        "  #\n",
        "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
        "  # want to consider the score with \"maximum context\", which we define as\n",
        "  # the *minimum* of its left and right context (the *sum* of left and\n",
        "  # right context will always be the same, of course).\n",
        "  #\n",
        "  # In the example the maximum context for 'bought' would be span C since\n",
        "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
        "  # and 0 right context.\n",
        "  best_score = None\n",
        "  best_span_index = None\n",
        "  for (span_index, doc_span) in enumerate(doc_spans):\n",
        "    end = doc_span.start + doc_span.length - 1\n",
        "    if position < doc_span.start:\n",
        "      continue\n",
        "    if position > end:\n",
        "      continue\n",
        "    num_left_context = position - doc_span.start\n",
        "    num_right_context = end - position\n",
        "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "    if best_score is None or score > best_score:\n",
        "      best_score = score\n",
        "      best_span_index = span_index\n",
        "\n",
        "  return cur_span_index == best_span_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF1JSALPSA07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAR_TRAIN_FILE_1 ='gs://bert_bucket_new/bert/files/paraphrased_train_squad_1.1_full_CD.json'\n",
        "par_examples_1 = read_squad_examples(\n",
        "    input_file=PAR_TRAIN_FILE_1, is_training=True)\n",
        "\n",
        "train_examples = read_squad_examples(\n",
        "    input_file=TRAIN_FILE, is_training=True)\n",
        "\n",
        "eval_examples = read_squad_examples(\n",
        "    input_file=EVAL_FILE, is_training=False)\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "    vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFBo_NIBc8t_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples_short = train_examples[:8560]\n",
        "par_examples_short = par_examples_1[:8560]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orRYCPO3U0GB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_examples = np.array([[ex for ex in train_examples_short+par_examples_short],\n",
        "                         [ex for ex in par_examples_short+train_examples_short]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZSYbvrxwOwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### old keep the same \n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Run BERT on SQuAD 1.1 and SQuAD 2.0.\"\"\"\n",
        "\n",
        "\n",
        "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
        "                         orig_answer_text):\n",
        "  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
        "\n",
        "  # The SQuAD annotations are character based. We first project them to\n",
        "  # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
        "  # often find a \"better match\". For example:\n",
        "  #\n",
        "  #   Question: What year was John Smith born?\n",
        "  #   Context: The leader was John Smith (1895-1943).\n",
        "  #   Answer: 1895\n",
        "  #\n",
        "  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
        "  # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
        "  # the exact answer, 1895.\n",
        "  #\n",
        "  # However, this is not always possible. Consider the following:\n",
        "  #\n",
        "  #   Question: What country is the top exporter of electornics?\n",
        "  #   Context: The Japanese electronics industry is the lagest in the world.\n",
        "  #   Answer: Japan\n",
        "  #\n",
        "  # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
        "  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
        "  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
        "  # in SQuAD, but does happen.\n",
        "  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "\n",
        "  for new_start in range(input_start, input_end + 1):\n",
        "    for new_end in range(input_end, new_start - 1, -1):\n",
        "      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
        "      if text_span == tok_answer_text:\n",
        "        return (new_start, new_end)\n",
        "\n",
        "  return (input_start, input_end)\n",
        "\n",
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  final_hidden = model.get_sequence_output()\n",
        "\n",
        "  final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\n",
        "  batch_size = final_hidden_shape[0]\n",
        "  seq_length = final_hidden_shape[1]\n",
        "  hidden_size = final_hidden_shape[2]\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"cls/squad/output_weights\", [2, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
        "\n",
        "  final_hidden_matrix = tf.reshape(final_hidden,\n",
        "                                   [batch_size * seq_length, hidden_size])\n",
        "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
        "  logits = tf.nn.bias_add(logits, output_bias)\n",
        "\n",
        "  logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
        "  logits = tf.transpose(logits, [2, 0, 1])\n",
        "\n",
        "  unstacked_logits = tf.unstack(logits, axis=0)\n",
        "\n",
        "  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
        "\n",
        "  return (start_logits, end_logits)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    unique_ids = features[\"unique_ids\"]\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (start_logits, end_logits) = create_model(\n",
        "        bert_config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      seq_length = modeling.get_shape_list(input_ids)[1]\n",
        "\n",
        "      def compute_loss(logits, positions):\n",
        "        one_hot_positions = tf.one_hot(\n",
        "            positions, depth=seq_length, dtype=tf.float32)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        loss = -tf.reduce_mean(\n",
        "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "        return loss\n",
        "\n",
        "      start_positions = features[\"start_positions\"]\n",
        "      end_positions = features[\"end_positions\"]\n",
        "\n",
        "      start_loss = compute_loss(start_logits, start_positions)\n",
        "      end_loss = compute_loss(end_logits, end_positions)\n",
        "\n",
        "      total_loss = (start_loss + end_loss) / 2.0\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      predictions = {\n",
        "          \"unique_ids\": unique_ids,\n",
        "          \"start_logits\": start_logits,\n",
        "          \"end_logits\": end_logits,\n",
        "      }\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8z0iquVznNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### new ... \n",
        "\n",
        "def kl(log_p, log_q):\n",
        "  neg_ent = tf.reduce_sum(tf.exp(log_p) * log_p, axis=-1)\n",
        "  neg_cross_ent = tf.reduce_sum(tf.exp(log_p) * log_q, axis=-1)\n",
        "\n",
        "  kl = neg_ent - neg_cross_ent\n",
        "  return kl\n",
        "\n",
        "def js(log_p, log_q):\n",
        "  log_m = 0.5 * (log_p + log_q)\n",
        "  \n",
        "  js = 0.5*kl(log_p, log_m) + 0.5*kl(log_q, log_m)\n",
        "  \n",
        "  return js\n",
        "\n",
        "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
        "                         orig_answer_text):\n",
        "  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
        "\n",
        "  # The SQuAD annotations are character based. We first project them to\n",
        "  # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
        "  # often find a \"better match\". For example:\n",
        "  #\n",
        "  #   Question: What year was John Smith born?\n",
        "  #   Context: The leader was John Smith (1895-1943).\n",
        "  #   Answer: 1895\n",
        "  #\n",
        "  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
        "  # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
        "  # the exact answer, 1895.\n",
        "  #\n",
        "  # However, this is not always possible. Consider the following:\n",
        "  #\n",
        "  #   Question: What country is the top exporter of electornics?\n",
        "  #   Context: The Japanese electronics industry is the lagest in the world.\n",
        "  #   Answer: Japan\n",
        "  #\n",
        "  # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
        "  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
        "  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
        "  # in SQuAD, but does happen.\n",
        "  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "\n",
        "  for new_start in range(input_start, input_end + 1):\n",
        "    for new_end in range(input_end, new_start - 1, -1):\n",
        "      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
        "      if text_span == tok_answer_text:\n",
        "        return (new_start, new_end)\n",
        "\n",
        "  return (input_start, input_end)\n",
        "\n",
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  hidden = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        " \n",
        "  final_hidden = hidden.get_sequence_output()\n",
        "  final_hidden_shape = modeling.get_shape_list(final_hidden)#, expected_rank=3)\n",
        "  batch_size = final_hidden_shape[0]\n",
        "  seq_length = final_hidden_shape[1]\n",
        "  hidden_size = final_hidden_shape[-1]\n",
        "  \n",
        "  print(batch_size) # 3 - > K * 3\n",
        "  print(seq_length) # 384 -> K x 384 \n",
        "  print(hidden_size) # 1024 \n",
        "  \n",
        "  with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
        "    output_weights = tf.get_variable(\n",
        "        \"cls/squad/output_weights\", [2, hidden_size],\n",
        "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
        "    output_bias = tf.get_variable(\n",
        "        \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
        "  \n",
        "  final_hidden_matrix = tf.reshape(final_hidden,\n",
        "                                   [batch_size * seq_length, hidden_size])\n",
        "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
        "  all_logits = tf.nn.bias_add(logits, output_bias)\n",
        "  \n",
        "  return all_logits, batch_size # return all logits for one example \n",
        "\n",
        "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    unique_ids = features[\"unique_ids\"]\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    \n",
        "    unique_ids_1 = features[\"unique_ids_1\"]\n",
        "    input_ids_1 = features[\"input_ids_1\"]\n",
        "    input_mask_1 = features[\"input_mask_1\"]\n",
        "    segment_ids_1 = features[\"segment_ids_1\"]\n",
        "\n",
        "    input_ids = tf.concat([\n",
        "          features[\"input_ids\"],\n",
        "          features[\"input_ids_1\"]], 0)\n",
        "    \n",
        "    input_mask = tf.concat([\n",
        "          features[\"input_mask\"],\n",
        "          features[\"input_mask_1\"]], 0)\n",
        "    \n",
        "    segment_ids = tf.concat([\n",
        "          features[\"segment_ids\"],\n",
        "          features[\"segment_ids_1\"]], 0)\n",
        "    \n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    all_logits, batch_size = create_model(\n",
        "        bert_config=bert_config,\n",
        "        is_training=is_training,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        segment_ids=segment_ids,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "    \n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    seq = 386\n",
        "    \n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "      seq_length = modeling.get_shape_list(input_ids)[1]\n",
        "\n",
        "      def compute_loss(logits, positions):\n",
        "        one_hot_positions = tf.one_hot(\n",
        "            positions, depth=seq, dtype=tf.float32)\n",
        "        \n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        loss = -tf.reduce_mean(\n",
        "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
        "        return loss\n",
        "\n",
        "      first_logits = all_logits[:int(batch_size/2)*seq]\n",
        "      first_logits = tf.reshape(first_logits, [int(batch_size/2), seq, 2])\n",
        "      first_logits = tf.transpose(first_logits, [2, 0, 1])\n",
        "\n",
        "      unstacked_logits = tf.unstack(first_logits, axis=0)\n",
        "\n",
        "      (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
        "\n",
        "      start_positions = features[\"start_positions\"]\n",
        "      end_positions = features[\"end_positions\"]\n",
        "\n",
        "      start_loss = compute_loss(start_logits, start_positions) \n",
        "      end_loss = compute_loss(end_logits, end_positions)\n",
        "\n",
        "      sup_loss = (start_loss + end_loss) / 2.0 # supervised loss per one example \n",
        "\n",
        "      print(\"PROCESSED SUPERVISZED LOSS\")\n",
        "\n",
        "      # unsupervised loss \n",
        "\n",
        "      if (\"unique_ids_1\") in features:\n",
        "      \n",
        "        # separate logits  \n",
        "        first_logits = all_logits[:int(batch_size/2)*seq_length]\n",
        "        second_logits = all_logits[int(batch_size/2)*seq_length:2*int(batch_size/2)*seq_length]\n",
        "        \n",
        "        # for each get start and end logits  \n",
        "        first_logits = tf.reshape(first_logits, [int(batch_size/2), seq, 2])\n",
        "        first_logits = tf.transpose(first_logits, [2, 0, 1])\n",
        "\n",
        "        unstacked_logits_0 = tf.unstack(first_logits, axis=0)\n",
        "\n",
        "        (start_logits_0, end_logits_0) = (unstacked_logits_0[0], unstacked_logits_0[1]) \n",
        "\n",
        "        second_logits = tf.reshape(second_logits, [int(batch_size/2), seq, 2])\n",
        "        second_logits = tf.transpose(second_logits, [2, 0, 1])\n",
        "\n",
        "        unstacked_logits_1 = tf.unstack(second_logits, axis=0)\n",
        "\n",
        "        (start_logits_1, end_logits_1) = (unstacked_logits_1[0], unstacked_logits_1[1]) \n",
        "\n",
        "        \n",
        "        # convert to probabilities \n",
        "        start_prob_0 = tf.nn.log_softmax(start_logits_0, axis=-1)\n",
        "        end_prob_0 = tf.nn.log_softmax(end_logits_0, axis=-1)\n",
        "        start_prob_1 = tf.nn.log_softmax(start_logits_1, axis=-1)\n",
        "        end_prob_1 = tf.nn.log_softmax(end_logits_1, axis=-1)\n",
        "        \n",
        "        # KL divergence / summetrified KL divergence\n",
        "        unsup_loss = tf.reduce_mean(js(start_prob_0, start_prob_1))# + tf.reduce_mean(kl(start_prob_1, start_prob_0))) / 2.0\n",
        "        \n",
        "        alpha = 0.5\n",
        "        \n",
        "        print(\"PROCESSED UNSUP LOSS\")\n",
        "    \n",
        "        # total loss \n",
        "        total_loss = alpha*(sup_loss) + (1-alpha)*unsup_loss\n",
        "         \n",
        "      else:\n",
        "        \n",
        "        total_loss = sup_loss\n",
        "     \n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "      \n",
        "      print(\"PROCESSED TRAIN OP\")\n",
        "      \n",
        "  \n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "      \n",
        "      print(\"OUTPUT_SPEC is done\")\n",
        "      \n",
        "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      \n",
        "      predictions = {\n",
        "          \"unique_ids\": unique_ids,\n",
        "          \"start_logits\": start_logits,\n",
        "          \"end_logits\": end_logits,\n",
        "      }\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADYKOO9UwiyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "  \n",
        "  name_to_features = {\n",
        "      \"unique_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \n",
        "      \"unique_ids_1\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"input_ids_1\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask_1\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids_1\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \n",
        "  }\n",
        "  \n",
        "  if is_training:\n",
        "    name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
        "    name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
        "  \n",
        "    name_to_features[\"start_positions_1\"] = tf.FixedLenFeature([], tf.int64)\n",
        "    name_to_features[\"end_positions_1\"] = tf.FixedLenFeature([], tf.int64)\n",
        "    \n",
        " \n",
        "  def _decode_record(record, name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.parse_single_example(record, name_to_features)\n",
        "    print(\"Example: \" + str(example))\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.to_int32(t)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    # For training, we want a lot of parallel reading and shuffling.\n",
        "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "    d = tf.data.TFRecordDataset(input_file)\n",
        "    d.repeat()\n",
        "    #d = StreamingFilesDataset(input_file)\n",
        "    \n",
        "    d = d.apply(\n",
        "        tf.contrib.data.map_and_batch(\n",
        "            lambda record: _decode_record(record, name_to_features),\n",
        "            batch_size=batch_size,\n",
        "            drop_remainder=drop_remainder))\n",
        "    \n",
        "    return d\n",
        "  \n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Ns53pCwxp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
        "\n",
        "\n",
        "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
        "                      max_answer_length, do_lower_case, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file):\n",
        "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
        "  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "  example_index_to_features = collections.defaultdict(list)\n",
        "  for feature in all_features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "  unique_id_to_result = {}\n",
        "  for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "      \"PrelimPrediction\",\n",
        "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "  all_predictions = collections.OrderedDict()\n",
        "  all_nbest_json = collections.OrderedDict()\n",
        "  scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "  for (example_index, example) in enumerate(all_examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "    # keep track of the minimum score of null start+end of position 0\n",
        "    score_null = 1000000  # large and positive\n",
        "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
        "    null_start_logit = 0  # the start logit at the slice with min null score\n",
        "    null_end_logit = 0  # the end logit at the slice with min null score\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "      result = unique_id_to_result[feature.unique_id]\n",
        "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "      # if we could have irrelevant answers, get the min score of irrelevant\n",
        "      if WITH_NEGATIVE:\n",
        "        feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
        "        if feature_null_score < score_null:\n",
        "          score_null = feature_null_score\n",
        "          min_null_feature_index = feature_index\n",
        "          null_start_logit = result.start_logits[0]\n",
        "          null_end_logit = result.end_logits[0]\n",
        "      for start_index in start_indexes:\n",
        "        for end_index in end_indexes:\n",
        "          # We could hypothetically create invalid predictions, e.g., predict\n",
        "          # that the start of the span is in the question. We throw out all\n",
        "          # invalid predictions.\n",
        "          if start_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if end_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if start_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if end_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if not feature.token_is_max_context.get(start_index, False):\n",
        "            continue\n",
        "          if end_index < start_index:\n",
        "            continue\n",
        "          length = end_index - start_index + 1\n",
        "          if length > max_answer_length:\n",
        "            continue\n",
        "          prelim_predictions.append(\n",
        "              _PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_logit=result.start_logits[start_index],\n",
        "                  end_logit=result.end_logits[end_index]))\n",
        "\n",
        "    if WITH_NEGATIVE:\n",
        "      prelim_predictions.append(\n",
        "          _PrelimPrediction(\n",
        "              feature_index=min_null_feature_index,\n",
        "              start_index=0,\n",
        "              end_index=0,\n",
        "              start_logit=null_start_logit,\n",
        "              end_logit=null_end_logit))\n",
        "    prelim_predictions = sorted(\n",
        "        prelim_predictions,\n",
        "        key=lambda x: (x.start_logit + x.end_logit),\n",
        "        reverse=True)\n",
        "\n",
        "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "      if len(nbest) >= n_best_size:\n",
        "        break\n",
        "      feature = features[pred.feature_index]\n",
        "      if pred.start_index > 0:  # this is a non-null prediction\n",
        "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "        tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "        # De-tokenize WordPieces that have been split off.\n",
        "        tok_text = tok_text.replace(\" ##\", \"\")\n",
        "        tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "        # Clean whitespace\n",
        "        tok_text = tok_text.strip()\n",
        "        tok_text = \" \".join(tok_text.split())\n",
        "        orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
        "        if final_text in seen_predictions:\n",
        "          continue\n",
        "\n",
        "        seen_predictions[final_text] = True\n",
        "      else:\n",
        "        final_text = \"\"\n",
        "        seen_predictions[final_text] = True\n",
        "\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=final_text,\n",
        "              start_logit=pred.start_logit,\n",
        "              end_logit=pred.end_logit))\n",
        "\n",
        "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
        "    if WITH_NEGATIVE:\n",
        "      if \"\" not in seen_predictions:\n",
        "        nbest.append(\n",
        "            _NbestPrediction(\n",
        "                text=\"\", start_logit=null_start_logit,\n",
        "                end_logit=null_end_logit))\n",
        "    # In very rare edge cases we could have no valid predictions. So we\n",
        "    # just create a nonce prediction in this case to avoid failure.\n",
        "    if not nbest:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "    assert len(nbest) >= 1\n",
        "\n",
        "    total_scores = []\n",
        "    best_non_null_entry = None\n",
        "    for entry in nbest:\n",
        "      total_scores.append(entry.start_logit + entry.end_logit)\n",
        "      if not best_non_null_entry:\n",
        "        if entry.text:\n",
        "          best_non_null_entry = entry\n",
        "\n",
        "    probs = _compute_softmax(total_scores)\n",
        "\n",
        "    nbest_json = []\n",
        "    for (i, entry) in enumerate(nbest):\n",
        "      output = collections.OrderedDict()\n",
        "      output[\"text\"] = entry.text\n",
        "      output[\"probability\"] = probs[i]\n",
        "      output[\"start_logit\"] = entry.start_logit\n",
        "      output[\"end_logit\"] = entry.end_logit\n",
        "      nbest_json.append(output)\n",
        "\n",
        "    assert len(nbest_json) >= 1\n",
        "\n",
        "    if not WITH_NEGATIVE:\n",
        "      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
        "    else:\n",
        "      # predict \"\" iff the null score - the score of best non-null > threshold\n",
        "      score_diff = score_null - best_non_null_entry.start_logit - (\n",
        "          best_non_null_entry.end_logit)\n",
        "      scores_diff_json[example.qas_id] = score_diff\n",
        "      if score_diff > NULL_SCORE_DIFF_THRESHOLD:\n",
        "        all_predictions[example.qas_id] = \"\"\n",
        "      else:\n",
        "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "\n",
        "    all_nbest_json[example.qas_id] = nbest_json\n",
        "    \n",
        "  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "\n",
        "  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "\n",
        "  if WITH_NEGATIVE:\n",
        "    with tf.gfile.GFile(output_null_log_odds_file, \"w\") as writer:\n",
        "      writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
        "\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case):\n",
        "  \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "  # When we created the data, we kept track of the alignment between original\n",
        "  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
        "  # now `orig_text` contains the span of our original text corresponding to the\n",
        "  # span that we predicted.\n",
        "  #\n",
        "  # However, `orig_text` may contain extra characters that we don't want in\n",
        "  # our prediction.\n",
        "  #\n",
        "  # For example, let's say:\n",
        "  #   pred_text = steve smith\n",
        "  #   orig_text = Steve Smith's\n",
        "  #\n",
        "  # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
        "  #\n",
        "  # We don't want to return `pred_text` because it's already been normalized\n",
        "  # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
        "  # our tokenizer does additional normalization like stripping accent\n",
        "  # characters).\n",
        "  #\n",
        "  # What we really want to return is \"Steve Smith\".\n",
        "  #\n",
        "  # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
        "  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
        "  # can fail in certain cases in which case we just return `orig_text`.\n",
        "\n",
        "  def _strip_spaces(text):\n",
        "    ns_chars = []\n",
        "    ns_to_s_map = collections.OrderedDict()\n",
        "    for (i, c) in enumerate(text):\n",
        "      if c == \" \":\n",
        "        continue\n",
        "      ns_to_s_map[len(ns_chars)] = i\n",
        "      ns_chars.append(c)\n",
        "    ns_text = \"\".join(ns_chars)\n",
        "    return (ns_text, ns_to_s_map)\n",
        "\n",
        "  # We first tokenize `orig_text`, strip whitespace from the result\n",
        "  # and `pred_text`, and check if they are the same length. If they are\n",
        "  # NOT the same length, the heuristic has failed. If they are the same\n",
        "  # length, we assume the characters are one-to-one aligned.\n",
        "  tokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "  tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "  start_position = tok_text.find(pred_text)\n",
        "  if start_position == -1:\n",
        "    if VERBOSE_LOGGING:\n",
        "      tf.logging.info(\n",
        "          \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "    return orig_text\n",
        "  end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "  if len(orig_ns_text) != len(tok_ns_text):\n",
        "    if VERBOSE_LOGGING:\n",
        "      tf.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
        "                      orig_ns_text, tok_ns_text)\n",
        "    return orig_text\n",
        "\n",
        "  # We then project the characters in `pred_text` back to `orig_text` using\n",
        "  # the character-to-character alignment.\n",
        "  tok_s_to_ns_map = {}\n",
        "  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
        "    tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "  orig_start_position = None\n",
        "  if start_position in tok_s_to_ns_map:\n",
        "    ns_start_position = tok_s_to_ns_map[start_position]\n",
        "    if ns_start_position in orig_ns_to_s_map:\n",
        "      orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "  if orig_start_position is None:\n",
        "    if VERBOSE_LOGGING:\n",
        "      tf.logging.info(\"Couldn't map start position\")\n",
        "    return orig_text\n",
        "\n",
        "  orig_end_position = None\n",
        "  if end_position in tok_s_to_ns_map:\n",
        "    ns_end_position = tok_s_to_ns_map[end_position]\n",
        "    if ns_end_position in orig_ns_to_s_map:\n",
        "      orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "  if orig_end_position is None:\n",
        "    if VERBOSE_LOGGING:\n",
        "      tf.logging.info(\"Couldn't map end position\")\n",
        "    return orig_text\n",
        "\n",
        "  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "  return output_text\n",
        "\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes\n",
        "\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "  \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "  if not scores:\n",
        "    return []\n",
        "\n",
        "  max_score = None\n",
        "  for score in scores:\n",
        "    if max_score is None or score > max_score:\n",
        "      max_score = score\n",
        "\n",
        "  exp_scores = []\n",
        "  total_sum = 0.0\n",
        "  for score in scores:\n",
        "    x = math.exp(score - max_score)\n",
        "    exp_scores.append(x)\n",
        "    total_sum += x\n",
        "\n",
        "  probs = []\n",
        "  for score in exp_scores:\n",
        "    probs.append(score / total_sum)\n",
        "  return probs\n",
        "\n",
        "\n",
        "class FeatureWriter(object):\n",
        "  \"\"\"Writes InputFeature to TF example file.\"\"\"\n",
        "\n",
        "  def __init__(self, filename, is_training):\n",
        "    self.filename = filename\n",
        "    self.is_training = is_training\n",
        "    self.num_features = 0\n",
        "    self._writer = tf.python_io.TFRecordWriter(filename)\n",
        "\n",
        "  def process_feature(self, feature_matrix):\n",
        "    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\n",
        "    self.num_features += 1\n",
        "\n",
        "    def create_int_feature(values):\n",
        "      feature = tf.train.Feature(\n",
        "          int64_list=tf.train.Int64List(value=list(values)))\n",
        "      return feature\n",
        " \n",
        "    # all features for each example \n",
        " \n",
        "    features = collections.OrderedDict()\n",
        "    if self.is_training:\n",
        "      \n",
        "      features[\"unique_ids\"] = create_int_feature([feature_matrix[0].unique_id])\n",
        "      features[\"input_ids\"] = create_int_feature(feature_matrix[0].input_ids)\n",
        "      features[\"input_mask\"] = create_int_feature(feature_matrix[0].input_mask)\n",
        "      features[\"segment_ids\"] = create_int_feature(feature_matrix[0].segment_ids)\n",
        "\n",
        "      features[\"unique_ids_1\"] = create_int_feature([feature_matrix[1].unique_id])\n",
        "      features[\"input_ids_1\"] = create_int_feature(feature_matrix[1].input_ids)\n",
        "      features[\"input_mask_1\"] = create_int_feature(feature_matrix[1].input_mask)\n",
        "      features[\"segment_ids_1\"] = create_int_feature(feature_matrix[1].segment_ids)\n",
        "\n",
        "      features[\"start_positions\"] = create_int_feature([feature_matrix[0].start_position])\n",
        "      features[\"end_positions\"] = create_int_feature([feature_matrix[0].end_position])\n",
        "      \n",
        "      features[\"start_positions_1\"] = create_int_feature([feature_matrix[1].start_position])\n",
        "      features[\"end_positions_1\"] = create_int_feature([feature_matrix[1].end_position])\n",
        "      \n",
        "      impossible = 0\n",
        "      if feature_matrix[0].is_impossible:\n",
        "        impossible = 1\n",
        "      features[\"is_impossible\"] = create_int_feature([impossible])\n",
        "\n",
        "    else:  \n",
        "      \n",
        "      features[\"unique_ids\"] = create_int_feature([feature_matrix.unique_id])\n",
        "      features[\"input_ids\"] = create_int_feature(feature_matrix.input_ids)\n",
        "      features[\"input_mask\"] = create_int_feature(feature_matrix.input_mask)\n",
        "      features[\"segment_ids\"] = create_int_feature(feature_matrix.segment_ids)\n",
        "      \n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    self._writer.write(tf_example.SerializeToString())  \n",
        "      \n",
        "  def close(self):\n",
        "    self._writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haKxRh08-hqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    tf_random_seed=1,\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYCfTUeXhUiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_estimator(train_batch_size, lr, train_epochs, warmup_proportion, num_train_steps):\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=lr,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=train_batch_size,\n",
        "      predict_batch_size=EVAL_BATCH_SIZE)\n",
        "  return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0dQH_0zXzis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_input_fn(examples):\n",
        "    train_writer = FeatureWriter(\n",
        "      filename=os.path.join(OUTPUT_DIR, \"train.tf_record\"),\n",
        "      is_training=True)\n",
        "    \n",
        "    all_tokens_1 = convert_examples_to_features(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        doc_stride=DOC_STRIDE,\n",
        "        max_query_length=64,\n",
        "        is_training=True,\n",
        "        output_fn=train_writer.process_feature)\n",
        "\n",
        "    train_writer.close()\n",
        "\n",
        "    train_input_fn = input_fn_builder(\n",
        "        input_file = train_writer.filename,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=True,\n",
        "        drop_remainder=True)\n",
        "\n",
        "    return train_input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCv2xFXYFbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_eval_input_fn(examples):\n",
        "    eval_writer = FeatureWriter(\n",
        "        filename=os.path.join(OUTPUT_DIR, \"eval.tf_record\"),\n",
        "        is_training=False)\n",
        "    eval_features = []\n",
        "\n",
        "    def append_feature(feature):\n",
        "      eval_features.append(feature)\n",
        "      eval_writer.process_feature(feature)\n",
        "\n",
        "    all_tokens= convert_examples_to_features_eval(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        doc_stride=DOC_STRIDE,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        output_fn=append_feature)\n",
        "    \n",
        "    eval_writer.close()\n",
        "\n",
        "    predict_input_fn = input_fn_builder(\n",
        "        input_file=eval_writer.filename,\n",
        "        seq_length=MAX_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=True)\n",
        "    \n",
        "    return predict_input_fn, eval_features, all_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vfSKZbNVXRp",
        "colab_type": "code",
        "outputId": "46c1fc8c-474f-446e-e7f7-39e1a317e16b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NUM_TRAIN_EPOCHS = 1\n",
        "num_train_steps = int(len(train_examples_short) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "estimator = get_estimator(TRAIN_BATCH_SIZE, LEARNING_RATE, NUM_TRAIN_EPOCHS, WARMUP_PROPORTION, num_train_steps)\n",
        "\n",
        "train_input_fn = get_train_input_fn(all_examples)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0905 15:31:53.110416 140003207100288 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f54ab9768c8>) includes params argument, but params are not passed to Estimator.\n",
            "I0905 15:31:53.113046 140003207100288 estimator.py:209] Using config: {'_model_dir': 'gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.94.95.10:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f54975bfb00>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.94.95.10:8470', '_evaluation_master': 'grpc://10.94.95.10:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f547bf507f0>}\n",
            "I0905 15:31:53.116009 140003207100288 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0905 15:31:53.122125 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.130825 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000000\n",
            "I0905 15:31:53.133675 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 0\n",
            "I0905 15:31:53.134572 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.135668 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] to whom did the virgin mary allegedly appear in 1858 in lou ##rdes france ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:31:53.137177 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\n",
            "I0905 15:31:53.139484 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0905 15:31:53.141039 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2000 3183 2106 1996 6261 2984 9382 3711 1999 8517 1999 10223 26371 2605 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.142268 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.143723 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.144852 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 130\n",
            "I0905 15:31:53.146230 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 137\n",
            "I0905 15:31:53.148695 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: saint bern ##ade ##tte so ##ub ##iro ##us\n",
            "I0905 15:31:53.159739 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.161504 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000001\n",
            "I0905 15:31:53.162864 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 1\n",
            "I0905 15:31:53.164226 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.165360 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is in front of the notre dame main building ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:31:53.166831 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 13:0 14:0 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:6 23:7 24:8 25:9 26:10 27:10 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:18 37:19 38:20 39:20 40:21 41:22 42:23 43:24 44:25 45:26 46:27 47:28 48:29 49:30 50:30 51:31 52:32 53:33 54:34 55:35 56:36 57:37 58:38 59:39 60:39 61:39 62:40 63:41 64:42 65:43 66:43 67:43 68:43 69:44 70:45 71:46 72:46 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:54 83:55 84:56 85:57 86:58 87:58 88:59 89:60 90:61 91:62 92:63 93:64 94:65 95:65 96:65 97:66 98:67 99:68 100:69 101:70 102:71 103:72 104:72 105:73 106:74 107:75 108:76 109:77 110:78 111:79 112:79 113:80 114:81 115:81 116:81 117:82 118:83 119:84 120:85 121:86 122:87 123:87 124:88 125:89 126:90 127:91 128:91 129:91 130:92 131:92 132:92 133:92 134:93 135:94 136:94 137:95 138:96 139:97 140:98 141:99 142:100 143:101 144:102 145:102 146:103 147:104 148:105 149:106 150:107 151:108 152:109 153:110 154:111 155:112 156:113 157:114 158:115 159:115 160:115 161:116 162:117 163:118 164:118 165:119 166:120 167:121 168:122 169:123 170:123\n",
            "I0905 15:31:53.169457 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True\n",
            "I0905 15:31:53.170696 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1999 2392 1997 1996 10289 8214 2364 2311 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.172253 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.173763 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.176067 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 52\n",
            "I0905 15:31:53.177398 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 56\n",
            "I0905 15:31:53.178718 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: a copper statue of christ\n",
            "I0905 15:31:53.189080 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.190861 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000002\n",
            "I0905 15:31:53.192755 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 2\n",
            "I0905 15:31:53.193728 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.196448 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] the basilica of the sacred heart at notre dame is beside to which structure ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:31:53.197942 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\n",
            "I0905 15:31:53.199364 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0905 15:31:53.201818 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 1996 13546 1997 1996 6730 2540 2012 10289 8214 2003 3875 2000 2029 3252 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.203110 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.204210 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.205381 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 81\n",
            "I0905 15:31:53.206339 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 83\n",
            "I0905 15:31:53.207376 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the main building\n",
            "I0905 15:31:53.214940 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.216582 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000003\n",
            "I0905 15:31:53.217744 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 3\n",
            "I0905 15:31:53.218883 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.220177 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is the gr ##otto at notre dame ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:31:53.221231 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:0 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:6 21:7 22:8 23:9 24:10 25:10 26:10 27:11 28:12 29:13 30:14 31:15 32:16 33:17 34:18 35:19 36:20 37:20 38:21 39:22 40:23 41:24 42:25 43:26 44:27 45:28 46:29 47:30 48:30 49:31 50:32 51:33 52:34 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:43 65:43 66:43 67:44 68:45 69:46 70:46 71:46 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:54 81:55 82:56 83:57 84:58 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:72 103:73 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:81 113:81 114:81 115:82 116:83 117:84 118:85 119:86 120:87 121:87 122:88 123:89 124:90 125:91 126:91 127:91 128:92 129:92 130:92 131:92 132:93 133:94 134:94 135:95 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:102 144:103 145:104 146:105 147:106 148:107 149:108 150:109 151:110 152:111 153:112 154:113 155:114 156:115 157:115 158:115 159:116 160:117 161:118 162:118 163:119 164:120 165:121 166:122 167:123 168:123\n",
            "I0905 15:31:53.222658 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0905 15:31:53.224153 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1996 24665 23052 2012 10289 8214 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.225585 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.227005 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.228049 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 95\n",
            "I0905 15:31:53.229104 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 101\n",
            "I0905 15:31:53.230379 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: a marian place of prayer and reflection\n",
            "I0905 15:31:53.239478 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.241658 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000004\n",
            "I0905 15:31:53.242969 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 4\n",
            "I0905 15:31:53.244072 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.245437 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what sits on top of the main building at notre dame ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:31:53.246693 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 14:0 15:0 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:6 24:7 25:8 26:9 27:10 28:10 29:10 30:11 31:12 32:13 33:14 34:15 35:16 36:17 37:18 38:19 39:20 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:28 49:29 50:30 51:30 52:31 53:32 54:33 55:34 56:35 57:36 58:37 59:38 60:39 61:39 62:39 63:40 64:41 65:42 66:43 67:43 68:43 69:43 70:44 71:45 72:46 73:46 74:46 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:58 89:59 90:60 91:61 92:62 93:63 94:64 95:65 96:65 97:65 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:72 106:73 107:74 108:75 109:76 110:77 111:78 112:79 113:79 114:80 115:81 116:81 117:81 118:82 119:83 120:84 121:85 122:86 123:87 124:87 125:88 126:89 127:90 128:91 129:91 130:91 131:92 132:92 133:92 134:92 135:93 136:94 137:94 138:95 139:96 140:97 141:98 142:99 143:100 144:101 145:102 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:111 156:112 157:113 158:114 159:115 160:115 161:115 162:116 163:117 164:118 165:118 166:119 167:120 168:121 169:122 170:123 171:123\n",
            "I0905 15:31:53.248102 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "I0905 15:31:53.249947 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 7719 2006 2327 1997 1996 2364 2311 2012 10289 8214 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.251240 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.252413 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.253569 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 33\n",
            "I0905 15:31:53.254484 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 39\n",
            "I0905 15:31:53.255731 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: a golden statue of the virgin mary\n",
            "I0905 15:31:53.268726 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.270631 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000005\n",
            "I0905 15:31:53.271894 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 5\n",
            "I0905 15:31:53.273118 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.274461 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] when did the scholastic magazine of notre dame begin publishing ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:31:53.275748 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:4 19:5 20:6 21:6 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:14 32:15 33:16 34:17 35:17 36:17 37:18 38:19 39:20 40:21 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:36 60:36 61:37 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:77 108:78 109:79 110:80 111:81 112:82 113:83 114:83 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:101 134:101 135:102 136:103 137:104 138:105 139:106 140:107 141:108 142:109 143:110 144:111 145:112 146:112 147:112 148:113 149:113 150:114 151:115 152:116 153:117 154:118 155:118 156:119 157:120 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:130 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:138 177:139 178:140 179:140 180:141 181:142 182:143 183:144 184:145 185:146 186:147 187:148 188:149 189:150 190:151 191:152 192:153 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:160 202:160 203:161 204:161 205:162 206:163 207:163 208:164 209:165 210:166 211:167 212:168 213:169 214:170 215:171 216:172 217:173 218:174 219:174 220:175 221:176 222:177 223:178 224:179 225:180 226:181 227:182 228:182 229:183 230:184 231:185 232:186 233:187 234:188 235:189 236:190 237:191 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:197 246:198 247:199 248:199 249:200 250:200 251:201 252:202 253:203 254:204 255:205 256:206 257:207 258:208 259:209 260:210 261:210 262:211 263:212 264:212 265:213 266:214 267:215 268:215\n",
            "I0905 15:31:53.277108 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True\n",
            "I0905 15:31:53.278575 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2043 2106 1996 24105 2932 1997 10289 8214 4088 4640 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.279966 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.281395 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.282675 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 63\n",
            "I0905 15:31:53.283756 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 64\n",
            "I0905 15:31:53.284862 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: september 1876\n",
            "I0905 15:31:53.296738 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.298302 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000006\n",
            "I0905 15:31:53.300163 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 6\n",
            "I0905 15:31:53.302992 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.304405 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] how often is notre dame ' s the jug ##gler published ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:31:53.306015 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 14:0 15:1 16:2 17:3 18:4 19:4 20:5 21:6 22:6 23:6 24:7 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:27 49:27 50:28 51:29 52:30 53:31 54:32 55:32 56:33 57:34 58:35 59:36 60:36 61:36 62:37 63:38 64:39 65:40 66:40 67:41 68:42 69:43 70:44 71:45 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:54 81:55 82:56 83:57 84:58 85:59 86:60 87:60 88:61 89:62 90:63 91:63 92:64 93:65 94:65 95:65 96:66 97:67 98:68 99:69 100:70 101:71 102:72 103:73 104:74 105:75 106:76 107:77 108:77 109:78 110:79 111:80 112:81 113:82 114:83 115:83 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:101 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:111 146:112 147:112 148:112 149:113 150:113 151:114 152:115 153:116 154:117 155:118 156:118 157:119 158:120 159:121 160:122 161:123 162:124 163:125 164:126 165:127 166:128 167:129 168:130 169:131 170:132 171:133 172:134 173:135 174:136 175:137 176:138 177:138 178:139 179:140 180:140 181:141 182:142 183:143 184:144 185:145 186:146 187:147 188:148 189:149 190:150 191:151 192:152 193:153 194:153 195:154 196:155 197:156 198:156 199:157 200:158 201:159 202:160 203:160 204:161 205:161 206:162 207:163 208:163 209:164 210:165 211:166 212:167 213:168 214:169 215:170 216:171 217:172 218:173 219:174 220:174 221:175 222:176 223:177 224:178 225:179 226:180 227:181 228:182 229:182 230:183 231:184 232:185 233:186 234:187 235:188 236:189 237:190 238:191 239:191 240:192 241:192 242:193 243:194 244:195 245:196 246:197 247:198 248:199 249:199 250:200 251:200 252:201 253:202 254:203 255:204 256:205 257:206 258:207 259:208 260:209 261:210 262:210 263:211 264:212 265:212 266:213 267:214 268:215 269:215\n",
            "I0905 15:31:53.307557 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True\n",
            "I0905 15:31:53.309074 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2129 2411 2003 10289 8214 1005 1055 1996 26536 17420 2405 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.310418 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.311855 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.313186 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 98\n",
            "I0905 15:31:53.315064 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 98\n",
            "I0905 15:31:53.316145 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: twice\n",
            "I0905 15:31:53.328508 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.330630 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000007\n",
            "I0905 15:31:53.331989 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 7\n",
            "I0905 15:31:53.333067 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.334404 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is the daily student paper at notre dame called ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:31:53.336313 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:4 19:5 20:6 21:6 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:14 32:15 33:16 34:17 35:17 36:17 37:18 38:19 39:20 40:21 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:36 60:36 61:37 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:77 108:78 109:79 110:80 111:81 112:82 113:83 114:83 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:101 134:101 135:102 136:103 137:104 138:105 139:106 140:107 141:108 142:109 143:110 144:111 145:112 146:112 147:112 148:113 149:113 150:114 151:115 152:116 153:117 154:118 155:118 156:119 157:120 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:130 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:138 177:139 178:140 179:140 180:141 181:142 182:143 183:144 184:145 185:146 186:147 187:148 188:149 189:150 190:151 191:152 192:153 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:160 202:160 203:161 204:161 205:162 206:163 207:163 208:164 209:165 210:166 211:167 212:168 213:169 214:170 215:171 216:172 217:173 218:174 219:174 220:175 221:176 222:177 223:178 224:179 225:180 226:181 227:182 228:182 229:183 230:184 231:185 232:186 233:187 234:188 235:189 236:190 237:191 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:197 246:198 247:199 248:199 249:200 250:200 251:201 252:202 253:203 254:204 255:205 256:206 257:207 258:208 259:209 260:210 261:210 262:211 263:212 264:212 265:213 266:214 267:215 268:215\n",
            "I0905 15:31:53.338143 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True\n",
            "I0905 15:31:53.339434 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1996 3679 3076 3259 2012 10289 8214 2170 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.340954 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.342492 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.344111 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 123\n",
            "I0905 15:31:53.345642 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 124\n",
            "I0905 15:31:53.346713 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the observer\n",
            "I0905 15:31:53.359562 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.361135 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000008\n",
            "I0905 15:31:53.362266 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 8\n",
            "I0905 15:31:53.363924 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.365147 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] how many student news papers are found at notre dame ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:31:53.366620 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:4 19:5 20:6 21:6 22:6 23:7 24:8 25:9 26:10 27:11 28:12 29:13 30:14 31:14 32:15 33:16 34:17 35:17 36:17 37:18 38:19 39:20 40:21 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:36 60:36 61:37 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:60 87:61 88:62 89:63 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:73 103:74 104:75 105:76 106:77 107:77 108:78 109:79 110:80 111:81 112:82 113:83 114:83 115:84 116:85 117:86 118:87 119:88 120:89 121:89 122:90 123:91 124:92 125:93 126:94 127:95 128:96 129:97 130:98 131:99 132:100 133:101 134:101 135:102 136:103 137:104 138:105 139:106 140:107 141:108 142:109 143:110 144:111 145:112 146:112 147:112 148:113 149:113 150:114 151:115 152:116 153:117 154:118 155:118 156:119 157:120 158:121 159:122 160:123 161:124 162:125 163:126 164:127 165:128 166:129 167:130 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:138 177:139 178:140 179:140 180:141 181:142 182:143 183:144 184:145 185:146 186:147 187:148 188:149 189:150 190:151 191:152 192:153 193:153 194:154 195:155 196:156 197:156 198:157 199:158 200:159 201:160 202:160 203:161 204:161 205:162 206:163 207:163 208:164 209:165 210:166 211:167 212:168 213:169 214:170 215:171 216:172 217:173 218:174 219:174 220:175 221:176 222:177 223:178 224:179 225:180 226:181 227:182 228:182 229:183 230:184 231:185 232:186 233:187 234:188 235:189 236:190 237:191 238:191 239:192 240:192 241:193 242:194 243:195 244:196 245:197 246:198 247:199 248:199 249:200 250:200 251:201 252:202 253:203 254:204 255:205 256:206 257:207 258:208 259:209 260:210 261:210 262:211 263:212 264:212 265:213 266:214 267:215 268:215\n",
            "I0905 15:31:53.368353 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True\n",
            "I0905 15:31:53.370287 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2129 2116 3076 2739 4981 2024 2179 2012 10289 8214 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.371612 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.373414 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.374551 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 39\n",
            "I0905 15:31:53.375626 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 39\n",
            "I0905 15:31:53.376963 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: three\n",
            "I0905 15:31:53.389862 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.391846 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000009\n",
            "I0905 15:31:53.392988 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 9\n",
            "I0905 15:31:53.394588 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.396005 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] in what year did the student paper common sense begin publication at notre dame ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:31:53.397723 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 17:0 18:1 19:2 20:3 21:4 22:4 23:5 24:6 25:6 26:6 27:7 28:8 29:9 30:10 31:11 32:12 33:13 34:14 35:14 36:15 37:16 38:17 39:17 40:17 41:18 42:19 43:20 44:21 45:21 46:22 47:23 48:24 49:25 50:26 51:27 52:27 53:28 54:29 55:30 56:31 57:32 58:32 59:33 60:34 61:35 62:36 63:36 64:36 65:37 66:38 67:39 68:40 69:40 70:41 71:42 72:43 73:44 74:45 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:59 89:60 90:60 91:61 92:62 93:63 94:63 95:64 96:65 97:65 98:65 99:66 100:67 101:68 102:69 103:70 104:71 105:72 106:73 107:74 108:75 109:76 110:77 111:77 112:78 113:79 114:80 115:81 116:82 117:83 118:83 119:84 120:85 121:86 122:87 123:88 124:89 125:89 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:111 149:112 150:112 151:112 152:113 153:113 154:114 155:115 156:116 157:117 158:118 159:118 160:119 161:120 162:121 163:122 164:123 165:124 166:125 167:126 168:127 169:128 170:129 171:130 172:131 173:132 174:133 175:134 176:135 177:136 178:137 179:138 180:138 181:139 182:140 183:140 184:141 185:142 186:143 187:144 188:145 189:146 190:147 191:148 192:149 193:150 194:151 195:152 196:153 197:153 198:154 199:155 200:156 201:156 202:157 203:158 204:159 205:160 206:160 207:161 208:161 209:162 210:163 211:163 212:164 213:165 214:166 215:167 216:168 217:169 218:170 219:171 220:172 221:173 222:174 223:174 224:175 225:176 226:177 227:178 228:179 229:180 230:181 231:182 232:182 233:183 234:184 235:185 236:186 237:187 238:188 239:189 240:190 241:191 242:191 243:192 244:192 245:193 246:194 247:195 248:196 249:197 250:198 251:199 252:199 253:200 254:200 255:201 256:202 257:203 258:204 259:205 260:206 261:207 262:208 263:209 264:210 265:210 266:211 267:212 268:212 269:213 270:214 271:215 272:215\n",
            "I0905 15:31:53.399182 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True\n",
            "I0905 15:31:53.400992 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 1999 2054 2095 2106 1996 3076 3259 2691 3168 4088 4772 2012 10289 8214 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.402365 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.404233 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.405468 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 182\n",
            "I0905 15:31:53.406581 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 182\n",
            "I0905 15:31:53.408715 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: 1987\n",
            "I0905 15:31:53.417098 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.418596 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000010\n",
            "I0905 15:31:53.419953 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 10\n",
            "I0905 15:31:53.421054 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.422400 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] where is the headquarters of the congregation of the holy cross ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:31:53.423987 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 14:0 15:1 16:2 17:3 18:4 19:5 20:6 21:7 22:8 23:9 24:10 25:11 26:12 27:12 28:13 29:14 30:15 31:16 32:16 33:17 34:18 35:19 36:20 37:20 38:20 39:21 40:22 41:23 42:23 43:24 44:24 45:25 46:25 47:26 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:37 60:38 61:38 62:39 63:40 64:40 65:41 66:42 67:43 68:44 69:45 70:46 71:47 72:48 73:49 74:50 75:51 76:52 77:52 78:53 79:54 80:54 81:55 82:56 83:57 84:57 85:57 86:58 87:59 88:60 89:61 90:62 91:63 92:64 93:65 94:66 95:66 96:67 97:68 98:69 99:69 100:69 101:70 102:71 103:72 104:72 105:73 106:74 107:75 108:76 109:76 110:76 111:77 112:78 113:79 114:80 115:80 116:80 117:81 118:82 119:83 120:84 121:85 122:85 123:86 124:87 125:88 126:89 127:90 128:91 129:92 130:92 131:92 132:92 133:93 134:94 135:95 136:95 137:96 138:96 139:96 140:97 141:98 142:99 143:100 144:101 145:102 146:103 147:104 148:104 149:105 150:106 151:107 152:108 153:108 154:108 155:109 156:110 157:111 158:111\n",
            "I0905 15:31:53.425730 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True\n",
            "I0905 15:31:53.427858 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2073 2003 1996 4075 1997 1996 7769 1997 1996 4151 2892 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.429779 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.431564 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.432886 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 36\n",
            "I0905 15:31:53.434187 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 36\n",
            "I0905 15:31:53.435328 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: rome\n",
            "I0905 15:31:53.444132 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.445633 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000011\n",
            "I0905 15:31:53.447706 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 11\n",
            "I0905 15:31:53.449057 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.450475 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is the primary seminary of the congregation of the holy cross ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:31:53.452174 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:7 23:8 24:9 25:10 26:11 27:12 28:12 29:13 30:14 31:15 32:16 33:16 34:17 35:18 36:19 37:20 38:20 39:20 40:21 41:22 42:23 43:23 44:24 45:24 46:25 47:25 48:26 49:27 50:28 51:29 52:30 53:31 54:32 55:32 56:33 57:34 58:35 59:36 60:37 61:38 62:38 63:39 64:40 65:40 66:41 67:42 68:43 69:44 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:52 79:53 80:54 81:54 82:55 83:56 84:57 85:57 86:57 87:58 88:59 89:60 90:61 91:62 92:63 93:64 94:65 95:66 96:66 97:67 98:68 99:69 100:69 101:69 102:70 103:71 104:72 105:72 106:73 107:74 108:75 109:76 110:76 111:76 112:77 113:78 114:79 115:80 116:80 117:80 118:81 119:82 120:83 121:84 122:85 123:85 124:86 125:87 126:88 127:89 128:90 129:91 130:92 131:92 132:92 133:92 134:93 135:94 136:95 137:95 138:96 139:96 140:96 141:97 142:98 143:99 144:100 145:101 146:102 147:103 148:104 149:104 150:105 151:106 152:107 153:108 154:108 155:108 156:109 157:110 158:111 159:111\n",
            "I0905 15:31:53.453688 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True\n",
            "I0905 15:31:53.455838 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1996 3078 8705 1997 1996 7769 1997 1996 4151 2892 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.457621 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.458996 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.460070 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 44\n",
            "I0905 15:31:53.461200 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 46\n",
            "I0905 15:31:53.462237 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: more ##au seminary\n",
            "I0905 15:31:53.470472 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.472796 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000012\n",
            "I0905 15:31:53.474122 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 12\n",
            "I0905 15:31:53.475227 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.476608 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is the oldest structure at notre dame ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:31:53.478031 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:16 30:17 31:18 32:19 33:20 34:20 35:20 36:21 37:22 38:23 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:38 59:39 60:40 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:52 75:53 76:54 77:54 78:55 79:56 80:57 81:57 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:68 95:69 96:69 97:69 98:70 99:71 100:72 101:72 102:73 103:74 104:75 105:76 106:76 107:76 108:77 109:78 110:79 111:80 112:80 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:92 128:92 129:92 130:93 131:94 132:95 133:95 134:96 135:96 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:107 149:108 150:108 151:108 152:109 153:110 154:111 155:111\n",
            "I0905 15:31:53.479233 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0905 15:31:53.481003 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1996 4587 3252 2012 10289 8214 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.482890 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.484764 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.486127 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 59\n",
            "I0905 15:31:53.487064 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 60\n",
            "I0905 15:31:53.488300 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: old college\n",
            "I0905 15:31:53.497007 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.498602 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000013\n",
            "I0905 15:31:53.499847 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 13\n",
            "I0905 15:31:53.501021 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.502378 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what individuals live at fatima house at notre dame ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:31:53.504058 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:12 26:13 27:14 28:15 29:16 30:16 31:17 32:18 33:19 34:20 35:20 36:20 37:21 38:22 39:23 40:23 41:24 42:24 43:25 44:25 45:26 46:27 47:28 48:29 49:30 50:31 51:32 52:32 53:33 54:34 55:35 56:36 57:37 58:38 59:38 60:39 61:40 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:52 76:53 77:54 78:54 79:55 80:56 81:57 82:57 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:68 96:69 97:69 98:69 99:70 100:71 101:72 102:72 103:73 104:74 105:75 106:76 107:76 108:76 109:77 110:78 111:79 112:80 113:80 114:80 115:81 116:82 117:83 118:84 119:85 120:85 121:86 122:87 123:88 124:89 125:90 126:91 127:92 128:92 129:92 130:92 131:93 132:94 133:95 134:95 135:96 136:96 137:96 138:97 139:98 140:99 141:100 142:101 143:102 144:103 145:104 146:104 147:105 148:106 149:107 150:108 151:108 152:108 153:109 154:110 155:111 156:111\n",
            "I0905 15:31:53.505290 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True\n",
            "I0905 15:31:53.506926 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 3633 2444 2012 27596 2160 2012 10289 8214 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.508307 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.509709 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.510958 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 84\n",
            "I0905 15:31:53.512510 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 87\n",
            "I0905 15:31:53.513756 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: retired priests and brothers\n",
            "I0905 15:31:53.522595 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.524079 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000014\n",
            "I0905 15:31:53.525166 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 14\n",
            "I0905 15:31:53.526471 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.527865 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] which prize did frederick bu ##ech ##ner create ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:31:53.529226 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:16 30:17 31:18 32:19 33:20 34:20 35:20 36:21 37:22 38:23 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:38 59:39 60:40 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:52 75:53 76:54 77:54 78:55 79:56 80:57 81:57 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:68 95:69 96:69 97:69 98:70 99:71 100:72 101:72 102:73 103:74 104:75 105:76 106:76 107:76 108:77 109:78 110:79 111:80 112:80 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:92 128:92 129:92 130:93 131:94 132:95 133:95 134:96 135:96 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:107 149:108 150:108 151:108 152:109 153:110 154:111 155:111\n",
            "I0905 15:31:53.530965 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0905 15:31:53.533047 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2029 3396 2106 5406 20934 15937 3678 3443 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.534688 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.536619 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.537749 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 149\n",
            "I0905 15:31:53.539706 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 154\n",
            "I0905 15:31:53.541449 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: bu ##ech ##ner prize for preaching\n",
            "I0905 15:31:53.550004 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.551784 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000015\n",
            "I0905 15:31:53.553102 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 15\n",
            "I0905 15:31:53.554480 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.555856 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] how many bs level degrees are offered in the college of engineering at notre dame ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:31:53.557185 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:7 27:8 28:8 29:9 30:10 31:11 32:12 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:26 48:27 49:28 50:29 51:29 52:30 53:31 54:32 55:33 56:33 57:34 58:34 59:34 60:35 61:36 62:36 63:36 64:36 65:36 66:36 67:36 68:37 69:38 70:39 71:39 72:40 73:41 74:42 75:43 76:44 77:45 78:46 79:47 80:48 81:49 82:49 83:50 84:51 85:52 86:52 87:52 88:52 89:53 90:53 91:54 92:55 93:56 94:57 95:58 96:58 97:59 98:60 99:61 100:62 101:62 102:63 103:64 104:65 105:66 106:67 107:68 108:69 109:69 110:69 111:69 112:70 113:71 114:71 115:72 116:72 117:73 118:74 119:75 120:76 121:76 122:76 123:77 124:78 125:79 126:80 127:81 128:82 129:83 130:84 131:85 132:86 133:87 134:88 135:89 136:90 137:91 138:92 139:92 140:92 141:92 142:93 143:94 144:95 145:96 146:97 147:98 148:98 149:98 150:99 151:99 152:100 153:100\n",
            "I0905 15:31:53.559685 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True\n",
            "I0905 15:31:53.561153 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2129 2116 18667 2504 5445 2024 3253 1999 1996 2267 1997 3330 2012 10289 8214 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.563291 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.564626 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.565654 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 107\n",
            "I0905 15:31:53.567349 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 107\n",
            "I0905 15:31:53.568924 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: eight\n",
            "I0905 15:31:53.576990 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.579037 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000016\n",
            "I0905 15:31:53.580294 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 16\n",
            "I0905 15:31:53.581385 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.582712 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] in what year was the college of engineering at notre dame formed ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:31:53.583830 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:7 23:7 24:8 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:16 34:17 35:18 36:19 37:20 38:21 39:22 40:23 41:24 42:25 43:26 44:26 45:27 46:28 47:29 48:29 49:30 50:31 51:32 52:33 53:33 54:34 55:34 56:34 57:35 58:36 59:36 60:36 61:36 62:36 63:36 64:36 65:37 66:38 67:39 68:39 69:40 70:41 71:42 72:43 73:44 74:45 75:46 76:47 77:48 78:49 79:49 80:50 81:51 82:52 83:52 84:52 85:52 86:53 87:53 88:54 89:55 90:56 91:57 92:58 93:58 94:59 95:60 96:61 97:62 98:62 99:63 100:64 101:65 102:66 103:67 104:68 105:69 106:69 107:69 108:69 109:70 110:71 111:71 112:72 113:72 114:73 115:74 116:75 117:76 118:76 119:76 120:77 121:78 122:79 123:80 124:81 125:82 126:83 127:84 128:85 129:86 130:87 131:88 132:89 133:90 134:91 135:92 136:92 137:92 138:92 139:93 140:94 141:95 142:96 143:97 144:98 145:98 146:98 147:99 148:99 149:100 150:100\n",
            "I0905 15:31:53.585140 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True\n",
            "I0905 15:31:53.587039 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 1999 2054 2095 2001 1996 2267 1997 3330 2012 10289 8214 2719 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.588346 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.589568 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.590977 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 22\n",
            "I0905 15:31:53.592739 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 22\n",
            "I0905 15:31:53.594051 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: 1920\n",
            "I0905 15:31:53.603303 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.605231 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000017\n",
            "I0905 15:31:53.607734 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 17\n",
            "I0905 15:31:53.608836 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.610707 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] before the creation of the college of engineering similar studies were carried out at which notre dame college ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:31:53.612119 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 21:0 22:1 23:2 24:3 25:4 26:5 27:6 28:7 29:7 30:8 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:15 39:16 40:17 41:18 42:19 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:26 51:27 52:28 53:29 54:29 55:30 56:31 57:32 58:33 59:33 60:34 61:34 62:34 63:35 64:36 65:36 66:36 67:36 68:36 69:36 70:36 71:37 72:38 73:39 74:39 75:40 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:48 84:49 85:49 86:50 87:51 88:52 89:52 90:52 91:52 92:53 93:53 94:54 95:55 96:56 97:57 98:58 99:58 100:59 101:60 102:61 103:62 104:62 105:63 106:64 107:65 108:66 109:67 110:68 111:69 112:69 113:69 114:69 115:70 116:71 117:71 118:72 119:72 120:73 121:74 122:75 123:76 124:76 125:76 126:77 127:78 128:79 129:80 130:81 131:82 132:83 133:84 134:85 135:86 136:87 137:88 138:89 139:90 140:91 141:92 142:92 143:92 144:92 145:93 146:94 147:95 148:96 149:97 150:98 151:98 152:98 153:99 154:99 155:100 156:100\n",
            "I0905 15:31:53.613256 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True\n",
            "I0905 15:31:53.614614 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2077 1996 4325 1997 1996 2267 1997 3330 2714 2913 2020 3344 2041 2012 2029 10289 8214 2267 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.616288 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.617697 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.619410 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 43\n",
            "I0905 15:31:53.620787 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 46\n",
            "I0905 15:31:53.622421 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the college of science\n",
            "I0905 15:31:53.630706 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.632805 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000018\n",
            "I0905 15:31:53.634671 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 18\n",
            "I0905 15:31:53.635935 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.637494 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] how many departments are within the st ##ins ##on - re ##mic ##k hall of engineering ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:31:53.638865 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:7 27:7 28:8 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:27 50:28 51:29 52:29 53:30 54:31 55:32 56:33 57:33 58:34 59:34 60:34 61:35 62:36 63:36 64:36 65:36 66:36 67:36 68:36 69:37 70:38 71:39 72:39 73:40 74:41 75:42 76:43 77:44 78:45 79:46 80:47 81:48 82:49 83:49 84:50 85:51 86:52 87:52 88:52 89:52 90:53 91:53 92:54 93:55 94:56 95:57 96:58 97:58 98:59 99:60 100:61 101:62 102:62 103:63 104:64 105:65 106:66 107:67 108:68 109:69 110:69 111:69 112:69 113:70 114:71 115:71 116:72 117:72 118:73 119:74 120:75 121:76 122:76 123:76 124:77 125:78 126:79 127:80 128:81 129:82 130:83 131:84 132:85 133:86 134:87 135:88 136:89 137:90 138:91 139:92 140:92 141:92 142:92 143:93 144:94 145:95 146:96 147:97 148:98 149:98 150:98 151:99 152:99 153:100 154:100\n",
            "I0905 15:31:53.639949 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True\n",
            "I0905 15:31:53.641422 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2129 2116 7640 2024 2306 1996 2358 7076 2239 1011 2128 7712 2243 2534 1997 3330 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.642866 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.644315 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.645584 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 74\n",
            "I0905 15:31:53.646954 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 74\n",
            "I0905 15:31:53.648075 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: five\n",
            "I0905 15:31:53.656717 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:31:53.658581 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000000019\n",
            "I0905 15:31:53.660180 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 19\n",
            "I0905 15:31:53.662044 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:31:53.663159 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] the college of science began to offer civil engineering courses beginning at what time at notre dame ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:31:53.664459 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 20:0 21:1 22:2 23:3 24:4 25:5 26:6 27:7 28:7 29:8 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:23 46:24 47:25 48:26 49:26 50:27 51:28 52:29 53:29 54:30 55:31 56:32 57:33 58:33 59:34 60:34 61:34 62:35 63:36 64:36 65:36 66:36 67:36 68:36 69:36 70:37 71:38 72:39 73:39 74:40 75:41 76:42 77:43 78:44 79:45 80:46 81:47 82:48 83:49 84:49 85:50 86:51 87:52 88:52 89:52 90:52 91:53 92:53 93:54 94:55 95:56 96:57 97:58 98:58 99:59 100:60 101:61 102:62 103:62 104:63 105:64 106:65 107:66 108:67 109:68 110:69 111:69 112:69 113:69 114:70 115:71 116:71 117:72 118:72 119:73 120:74 121:75 122:76 123:76 124:76 125:77 126:78 127:79 128:80 129:81 130:82 131:83 132:84 133:85 134:86 135:87 136:88 137:89 138:90 139:91 140:92 141:92 142:92 143:92 144:93 145:94 146:95 147:96 148:97 149:98 150:98 151:98 152:99 153:99 154:100 155:100\n",
            "I0905 15:31:53.665783 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0905 15:31:53.667031 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 1996 2267 1997 2671 2211 2000 3749 2942 3330 5352 2927 2012 2054 2051 2012 10289 8214 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.668584 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.669960 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:31:53.671036 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 47\n",
            "I0905 15:31:53.672870 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 48\n",
            "I0905 15:31:53.674496 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the 1870s\n",
            "I0905 15:33:46.477663 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.479017 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034240\n",
            "I0905 15:33:46.480002 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 0\n",
            "I0905 15:33:46.480940 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.481905 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] it is said that the virgin mary appeared in lou ##rdes france in 1858 ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:33:46.482908 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\n",
            "I0905 15:33:46.483997 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0905 15:33:46.486637 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2009 2003 2056 2008 1996 6261 2984 2596 1999 10223 26371 2605 1999 8517 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.487852 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.489033 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.489934 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 130\n",
            "I0905 15:33:46.490824 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 137\n",
            "I0905 15:33:46.491729 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: saint bern ##ade ##tte so ##ub ##iro ##us\n",
            "I0905 15:33:46.498459 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.499624 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034241\n",
            "I0905 15:33:46.500622 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 1\n",
            "I0905 15:33:46.501649 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.502766 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is opposite the main building of notre dame de paris ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:33:46.503877 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 14:0 15:0 16:0 17:1 18:2 19:3 20:4 21:5 22:6 23:6 24:7 25:8 26:9 27:10 28:10 29:10 30:11 31:12 32:13 33:14 34:15 35:16 36:17 37:18 38:19 39:20 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:28 49:29 50:30 51:30 52:31 53:32 54:33 55:34 56:35 57:36 58:37 59:38 60:39 61:39 62:39 63:40 64:41 65:42 66:43 67:43 68:43 69:43 70:44 71:45 72:46 73:46 74:46 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:58 89:59 90:60 91:61 92:62 93:63 94:64 95:65 96:65 97:65 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:72 106:73 107:74 108:75 109:76 110:77 111:78 112:79 113:79 114:80 115:81 116:81 117:81 118:82 119:83 120:84 121:85 122:86 123:87 124:87 125:88 126:89 127:90 128:91 129:91 130:91 131:92 132:92 133:92 134:92 135:93 136:94 137:94 138:95 139:96 140:97 141:98 142:99 143:100 144:101 145:102 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:111 156:112 157:113 158:114 159:115 160:115 161:115 162:116 163:117 164:118 165:118 166:119 167:120 168:121 169:122 170:123 171:123\n",
            "I0905 15:33:46.504998 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "I0905 15:33:46.506198 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 4500 1996 2364 2311 1997 10289 8214 2139 3000 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.507372 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.508509 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.509595 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 53\n",
            "I0905 15:33:46.510651 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 57\n",
            "I0905 15:33:46.511699 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: a copper statue of christ\n",
            "I0905 15:33:46.518647 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.519804 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034242\n",
            "I0905 15:33:46.520819 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 2\n",
            "I0905 15:33:46.521844 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.522951 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] which structure is next to the cathedral of the sacred heart of notre dame de paris ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:33:46.524110 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 19:0 20:0 21:0 22:1 23:2 24:3 25:4 26:5 27:6 28:6 29:7 30:8 31:9 32:10 33:10 34:10 35:11 36:12 37:13 38:14 39:15 40:16 41:17 42:18 43:19 44:20 45:20 46:21 47:22 48:23 49:24 50:25 51:26 52:27 53:28 54:29 55:30 56:30 57:31 58:32 59:33 60:34 61:35 62:36 63:37 64:38 65:39 66:39 67:39 68:40 69:41 70:42 71:43 72:43 73:43 74:43 75:44 76:45 77:46 78:46 79:46 80:46 81:47 82:48 83:49 84:50 85:51 86:52 87:53 88:54 89:55 90:56 91:57 92:58 93:58 94:59 95:60 96:61 97:62 98:63 99:64 100:65 101:65 102:65 103:66 104:67 105:68 106:69 107:70 108:71 109:72 110:72 111:73 112:74 113:75 114:76 115:77 116:78 117:79 118:79 119:80 120:81 121:81 122:81 123:82 124:83 125:84 126:85 127:86 128:87 129:87 130:88 131:89 132:90 133:91 134:91 135:91 136:92 137:92 138:92 139:92 140:93 141:94 142:94 143:95 144:96 145:97 146:98 147:99 148:100 149:101 150:102 151:102 152:103 153:104 154:105 155:106 156:107 157:108 158:109 159:110 160:111 161:112 162:113 163:114 164:115 165:115 166:115 167:116 168:117 169:118 170:118 171:119 172:120 173:121 174:122 175:123 176:123\n",
            "I0905 15:33:46.525218 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True\n",
            "I0905 15:33:46.526432 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2029 3252 2003 2279 2000 1996 5040 1997 1996 6730 2540 1997 10289 8214 2139 3000 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.527649 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.528846 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.529903 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 83\n",
            "I0905 15:33:46.530938 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 85\n",
            "I0905 15:33:46.531990 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the main building\n",
            "I0905 15:33:46.538822 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.539957 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034243\n",
            "I0905 15:33:46.540984 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 3\n",
            "I0905 15:33:46.542016 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.545634 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is the notre dame de paris cave ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:33:46.546614 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:0 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:6 21:7 22:8 23:9 24:10 25:10 26:10 27:11 28:12 29:13 30:14 31:15 32:16 33:17 34:18 35:19 36:20 37:20 38:21 39:22 40:23 41:24 42:25 43:26 44:27 45:28 46:29 47:30 48:30 49:31 50:32 51:33 52:34 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:43 65:43 66:43 67:44 68:45 69:46 70:46 71:46 72:46 73:47 74:48 75:49 76:50 77:51 78:52 79:53 80:54 81:55 82:56 83:57 84:58 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:65 94:65 95:66 96:67 97:68 98:69 99:70 100:71 101:72 102:72 103:73 104:74 105:75 106:76 107:77 108:78 109:79 110:79 111:80 112:81 113:81 114:81 115:82 116:83 117:84 118:85 119:86 120:87 121:87 122:88 123:89 124:90 125:91 126:91 127:91 128:92 129:92 130:92 131:92 132:93 133:94 134:94 135:95 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:102 144:103 145:104 146:105 147:106 148:107 149:108 150:109 151:110 152:111 153:112 154:113 155:114 156:115 157:115 158:115 159:116 160:117 161:118 162:118 163:119 164:120 165:121 166:122 167:123 168:123\n",
            "I0905 15:33:46.547595 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0905 15:33:46.548568 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1996 10289 8214 2139 3000 5430 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.550078 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.551054 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.553383 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 95\n",
            "I0905 15:33:46.554149 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 101\n",
            "I0905 15:33:46.555475 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: a marian place of prayer and reflection\n",
            "I0905 15:33:46.574777 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.580039 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034244\n",
            "I0905 15:33:46.581409 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 4\n",
            "I0905 15:33:46.583374 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.584090 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is at the top of the main building of notre dame de paris ? [SEP] architectural ##ly , the school has a catholic character . atop the main building ' s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms up ##rai ##sed with the legend \" ve ##ni ##te ad me om ##nes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the gr ##otto , a marian place of prayer and reflection . it is a replica of the gr ##otto at lou ##rdes , france where the virgin mary reputed ##ly appeared to saint bern ##ade ##tte so ##ub ##iro ##us in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . [SEP]\n",
            "I0905 15:33:46.584912 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 17:0 18:0 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:6 27:7 28:8 29:9 30:10 31:10 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:20 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:39 65:39 66:40 67:41 68:42 69:43 70:43 71:43 72:43 73:44 74:45 75:46 76:46 77:46 78:46 79:47 80:48 81:49 82:50 83:51 84:52 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:79 116:79 117:80 118:81 119:81 120:81 121:82 122:83 123:84 124:85 125:86 126:87 127:87 128:88 129:89 130:90 131:91 132:91 133:91 134:92 135:92 136:92 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:98 145:99 146:100 147:101 148:102 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:115 164:115 165:116 166:117 167:118 168:118 169:119 170:120 171:121 172:122 173:123 174:123\n",
            "I0905 15:33:46.586013 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0905 15:33:46.587056 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 2012 1996 2327 1997 1996 2364 2311 1997 10289 8214 2139 3000 1029 102 6549 2135 1010 1996 2082 2038 1037 3234 2839 1012 10234 1996 2364 2311 1005 1055 2751 8514 2003 1037 3585 6231 1997 1996 6261 2984 1012 3202 1999 2392 1997 1996 2364 2311 1998 5307 2009 1010 2003 1037 6967 6231 1997 4828 2007 2608 2039 14995 6924 2007 1996 5722 1000 2310 3490 2618 4748 2033 18168 5267 1000 1012 2279 2000 1996 2364 2311 2003 1996 13546 1997 1996 6730 2540 1012 3202 2369 1996 13546 2003 1996 24665 23052 1010 1037 14042 2173 1997 7083 1998 9185 1012 2009 2003 1037 15059 1997 1996 24665 23052 2012 10223 26371 1010 2605 2073 1996 6261 2984 22353 2135 2596 2000 3002 16595 9648 4674 2061 12083 9711 2271 1999 8517 1012 2012 1996 2203 1997 1996 2364 3298 1006 1998 1999 1037 3622 2240 2008 8539 2083 1017 11342 1998 1996 2751 8514 1007 1010 2003 1037 3722 1010 2715 2962 6231 1997 2984 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.588475 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.589439 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.590626 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 36\n",
            "I0905 15:33:46.591777 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 42\n",
            "I0905 15:33:46.592694 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: a golden statue of the virgin mary\n",
            "I0905 15:33:46.608393 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.609440 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034245\n",
            "I0905 15:33:46.611064 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 5\n",
            "I0905 15:33:46.612209 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.613709 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] when did the scholastic magazine of the college of notre - dame begin to be published ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:33:46.615025 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 19:0 20:1 21:2 22:3 23:4 24:4 25:5 26:6 27:6 28:6 29:7 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:14 38:15 39:16 40:17 41:17 42:17 43:18 44:19 45:20 46:21 47:21 48:22 49:23 50:24 51:25 52:26 53:27 54:27 55:28 56:29 57:30 58:31 59:32 60:32 61:33 62:34 63:35 64:36 65:36 66:36 67:37 68:38 69:39 70:40 71:40 72:41 73:42 74:43 75:44 76:45 77:46 78:47 79:48 80:49 81:50 82:51 83:52 84:53 85:54 86:55 87:56 88:57 89:58 90:59 91:60 92:60 93:61 94:62 95:63 96:63 97:64 98:65 99:65 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:73 109:74 110:75 111:76 112:77 113:77 114:78 115:79 116:80 117:81 118:82 119:83 120:83 121:84 122:85 123:86 124:87 125:88 126:89 127:89 128:90 129:91 130:92 131:93 132:94 133:95 134:96 135:97 136:98 137:99 138:100 139:101 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:111 151:112 152:112 153:112 154:113 155:113 156:114 157:115 158:116 159:117 160:118 161:118 162:119 163:120 164:121 165:122 166:123 167:124 168:125 169:126 170:127 171:128 172:129 173:130 174:131 175:132 176:133 177:134 178:135 179:136 180:137 181:138 182:138 183:139 184:140 185:140 186:141 187:142 188:143 189:144 190:145 191:146 192:147 193:148 194:149 195:150 196:151 197:152 198:153 199:153 200:154 201:155 202:156 203:156 204:157 205:158 206:159 207:160 208:160 209:161 210:161 211:162 212:163 213:163 214:164 215:165 216:166 217:167 218:168 219:169 220:170 221:171 222:172 223:173 224:174 225:174 226:175 227:176 228:177 229:178 230:179 231:180 232:181 233:182 234:182 235:183 236:184 237:185 238:186 239:187 240:188 241:189 242:190 243:191 244:191 245:192 246:192 247:193 248:194 249:195 250:196 251:197 252:198 253:199 254:199 255:200 256:200 257:201 258:202 259:203 260:204 261:205 262:206 263:207 264:208 265:209 266:210 267:210 268:211 269:212 270:212 271:213 272:214 273:215 274:215\n",
            "I0905 15:33:46.616275 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True 272:True 273:True 274:True\n",
            "I0905 15:33:46.617815 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2043 2106 1996 24105 2932 1997 1996 2267 1997 10289 1011 8214 4088 2000 2022 2405 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.620346 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.622185 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.623570 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 69\n",
            "I0905 15:33:46.625232 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 70\n",
            "I0905 15:33:46.626459 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: september 1876\n",
            "I0905 15:33:46.642229 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.654612 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034246\n",
            "I0905 15:33:46.656033 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 6\n",
            "I0905 15:33:46.657512 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.658994 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] does notre dame often fall in jug ##gler ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:33:46.660671 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:4 17:5 18:6 19:6 20:6 21:7 22:8 23:9 24:10 25:11 26:12 27:13 28:14 29:14 30:15 31:16 32:17 33:17 34:17 35:18 36:19 37:20 38:21 39:21 40:22 41:23 42:24 43:25 44:26 45:27 46:27 47:28 48:29 49:30 50:31 51:32 52:32 53:33 54:34 55:35 56:36 57:36 58:36 59:37 60:38 61:39 62:40 63:40 64:41 65:42 66:43 67:44 68:45 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:53 77:54 78:55 79:56 80:57 81:58 82:59 83:60 84:60 85:61 86:62 87:63 88:63 89:64 90:65 91:65 92:65 93:66 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:75 103:76 104:77 105:77 106:78 107:79 108:80 109:81 110:82 111:83 112:83 113:84 114:85 115:86 116:87 117:88 118:89 119:89 120:90 121:91 122:92 123:93 124:94 125:95 126:96 127:97 128:98 129:99 130:100 131:101 132:101 133:102 134:103 135:104 136:105 137:106 138:107 139:108 140:109 141:110 142:111 143:112 144:112 145:112 146:113 147:113 148:114 149:115 150:116 151:117 152:118 153:118 154:119 155:120 156:121 157:122 158:123 159:124 160:125 161:126 162:127 163:128 164:129 165:130 166:131 167:132 168:133 169:134 170:135 171:136 172:137 173:138 174:138 175:139 176:140 177:140 178:141 179:142 180:143 181:144 182:145 183:146 184:147 185:148 186:149 187:150 188:151 189:152 190:153 191:153 192:154 193:155 194:156 195:156 196:157 197:158 198:159 199:160 200:160 201:161 202:161 203:162 204:163 205:163 206:164 207:165 208:166 209:167 210:168 211:169 212:170 213:171 214:172 215:173 216:174 217:174 218:175 219:176 220:177 221:178 222:179 223:180 224:181 225:182 226:182 227:183 228:184 229:185 230:186 231:187 232:188 233:189 234:190 235:191 236:191 237:192 238:192 239:193 240:194 241:195 242:196 243:197 244:198 245:199 246:199 247:200 248:200 249:201 250:202 251:203 252:204 253:205 254:206 255:207 256:208 257:209 258:210 259:210 260:211 261:212 262:212 263:213 264:214 265:215 266:215\n",
            "I0905 15:33:46.662173 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True\n",
            "I0905 15:33:46.663851 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2515 10289 8214 2411 2991 1999 26536 17420 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.665313 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.666950 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.668203 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 95\n",
            "I0905 15:33:46.669158 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 95\n",
            "I0905 15:33:46.670366 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: twice\n",
            "I0905 15:33:46.685145 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.686343 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034247\n",
            "I0905 15:33:46.688595 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 7\n",
            "I0905 15:33:46.690651 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.691958 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is notre dame ' s daily student day ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:33:46.694337 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:4 18:5 19:6 20:6 21:6 22:7 23:8 24:9 25:10 26:11 27:12 28:13 29:14 30:14 31:15 32:16 33:17 34:17 35:17 36:18 37:19 38:20 39:21 40:21 41:22 42:23 43:24 44:25 45:26 46:27 47:27 48:28 49:29 50:30 51:31 52:32 53:32 54:33 55:34 56:35 57:36 58:36 59:36 60:37 61:38 62:39 63:40 64:40 65:41 66:42 67:43 68:44 69:45 70:46 71:47 72:48 73:49 74:50 75:51 76:52 77:53 78:54 79:55 80:56 81:57 82:58 83:59 84:60 85:60 86:61 87:62 88:63 89:63 90:64 91:65 92:65 93:65 94:66 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:75 104:76 105:77 106:77 107:78 108:79 109:80 110:81 111:82 112:83 113:83 114:84 115:85 116:86 117:87 118:88 119:89 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:99 131:100 132:101 133:101 134:102 135:103 136:104 137:105 138:106 139:107 140:108 141:109 142:110 143:111 144:112 145:112 146:112 147:113 148:113 149:114 150:115 151:116 152:117 153:118 154:118 155:119 156:120 157:121 158:122 159:123 160:124 161:125 162:126 163:127 164:128 165:129 166:130 167:131 168:132 169:133 170:134 171:135 172:136 173:137 174:138 175:138 176:139 177:140 178:140 179:141 180:142 181:143 182:144 183:145 184:146 185:147 186:148 187:149 188:150 189:151 190:152 191:153 192:153 193:154 194:155 195:156 196:156 197:157 198:158 199:159 200:160 201:160 202:161 203:161 204:162 205:163 206:163 207:164 208:165 209:166 210:167 211:168 212:169 213:170 214:171 215:172 216:173 217:174 218:174 219:175 220:176 221:177 222:178 223:179 224:180 225:181 226:182 227:182 228:183 229:184 230:185 231:186 232:187 233:188 234:189 235:190 236:191 237:191 238:192 239:192 240:193 241:194 242:195 243:196 244:197 245:198 246:199 247:199 248:200 249:200 250:201 251:202 252:203 253:204 254:205 255:206 256:207 257:208 258:209 259:210 260:210 261:211 262:212 263:212 264:213 265:214 266:215 267:215\n",
            "I0905 15:33:46.695444 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True\n",
            "I0905 15:33:46.698335 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 10289 8214 1005 1055 3679 3076 2154 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.700125 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.702593 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.704384 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 122\n",
            "I0905 15:33:46.705698 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 123\n",
            "I0905 15:33:46.707014 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the observer\n",
            "I0905 15:33:46.722495 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.724172 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034248\n",
            "I0905 15:33:46.726641 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 8\n",
            "I0905 15:33:46.728406 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.730950 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] how many news newspapers for students have you found in notre dame ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:33:46.733140 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:4 21:5 22:6 23:6 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:14 34:15 35:16 36:17 37:17 38:17 39:18 40:19 41:20 42:21 43:21 44:22 45:23 46:24 47:25 48:26 49:27 50:27 51:28 52:29 53:30 54:31 55:32 56:32 57:33 58:34 59:35 60:36 61:36 62:36 63:37 64:38 65:39 66:40 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:47 75:48 76:49 77:50 78:51 79:52 80:53 81:54 82:55 83:56 84:57 85:58 86:59 87:60 88:60 89:61 90:62 91:63 92:63 93:64 94:65 95:65 96:65 97:66 98:67 99:68 100:69 101:70 102:71 103:72 104:73 105:74 106:75 107:76 108:77 109:77 110:78 111:79 112:80 113:81 114:82 115:83 116:83 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:101 136:101 137:102 138:103 139:104 140:105 141:106 142:107 143:108 144:109 145:110 146:111 147:112 148:112 149:112 150:113 151:113 152:114 153:115 154:116 155:117 156:118 157:118 158:119 159:120 160:121 161:122 162:123 163:124 164:125 165:126 166:127 167:128 168:129 169:130 170:131 171:132 172:133 173:134 174:135 175:136 176:137 177:138 178:138 179:139 180:140 181:140 182:141 183:142 184:143 185:144 186:145 187:146 188:147 189:148 190:149 191:150 192:151 193:152 194:153 195:153 196:154 197:155 198:156 199:156 200:157 201:158 202:159 203:160 204:160 205:161 206:161 207:162 208:163 209:163 210:164 211:165 212:166 213:167 214:168 215:169 216:170 217:171 218:172 219:173 220:174 221:174 222:175 223:176 224:177 225:178 226:179 227:180 228:181 229:182 230:182 231:183 232:184 233:185 234:186 235:187 236:188 237:189 238:190 239:191 240:191 241:192 242:192 243:193 244:194 245:195 246:196 247:197 248:198 249:199 250:199 251:200 252:200 253:201 254:202 255:203 256:204 257:205 258:206 259:207 260:208 261:209 262:210 263:210 264:211 265:212 266:212 267:213 268:214 269:215 270:215\n",
            "I0905 15:33:46.734698 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True\n",
            "I0905 15:33:46.736379 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2129 2116 2739 6399 2005 2493 2031 2017 2179 1999 10289 8214 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.737846 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.739479 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.740861 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 41\n",
            "I0905 15:33:46.742327 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 41\n",
            "I0905 15:33:46.744000 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: three\n",
            "I0905 15:33:46.759596 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.761240 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034249\n",
            "I0905 15:33:46.763191 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 9\n",
            "I0905 15:33:46.764582 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.766004 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] in which year did the common sense essay start publication in notre dame ? [SEP] as at most other universities , notre dame ' s students run a number of news media outlets . the nine student - run outlets include three newspapers , both a radio and television station , and several magazines and journals . begun as a one - page journal in september 1876 , the scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the united states . the other magazine , the jug ##gler , is released twice a year and focuses on student literature and artwork . the dome yearbook is published annually . the newspapers have varying publication interests , with the observer published daily and mainly reporting university and other news , and staffed by students from both notre dame and saint mary ' s college . unlike scholastic and the dome , the observer is an independent publication and does not have a faculty advisor or any editorial oversight from the university . in 1987 , when some students believed that the observer began to show a conservative bias , a liberal newspaper , common sense was published . likewise , in 2003 , when other students believed that the paper showed a liberal bias , the conservative paper irish rover went into production . neither paper is published as often as the observer ; however , all three are distributed to all students . finally , in spring 2008 an undergraduate journal for political science research , beyond politics , made its debut . [SEP]\n",
            "I0905 15:33:46.767198 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 16:0 17:1 18:2 19:3 20:4 21:4 22:5 23:6 24:6 25:6 26:7 27:8 28:9 29:10 30:11 31:12 32:13 33:14 34:14 35:15 36:16 37:17 38:17 39:17 40:18 41:19 42:20 43:21 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:27 52:28 53:29 54:30 55:31 56:32 57:32 58:33 59:34 60:35 61:36 62:36 63:36 64:37 65:38 66:39 67:40 68:40 69:41 70:42 71:43 72:44 73:45 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:54 83:55 84:56 85:57 86:58 87:59 88:60 89:60 90:61 91:62 92:63 93:63 94:64 95:65 96:65 97:65 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:74 107:75 108:76 109:77 110:77 111:78 112:79 113:80 114:81 115:82 116:83 117:83 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:111 148:112 149:112 150:112 151:113 152:113 153:114 154:115 155:116 156:117 157:118 158:118 159:119 160:120 161:121 162:122 163:123 164:124 165:125 166:126 167:127 168:128 169:129 170:130 171:131 172:132 173:133 174:134 175:135 176:136 177:137 178:138 179:138 180:139 181:140 182:140 183:141 184:142 185:143 186:144 187:145 188:146 189:147 190:148 191:149 192:150 193:151 194:152 195:153 196:153 197:154 198:155 199:156 200:156 201:157 202:158 203:159 204:160 205:160 206:161 207:161 208:162 209:163 210:163 211:164 212:165 213:166 214:167 215:168 216:169 217:170 218:171 219:172 220:173 221:174 222:174 223:175 224:176 225:177 226:178 227:179 228:180 229:181 230:182 231:182 232:183 233:184 234:185 235:186 236:187 237:188 238:189 239:190 240:191 241:191 242:192 243:192 244:193 245:194 246:195 247:196 248:197 249:198 250:199 251:199 252:200 253:200 254:201 255:202 256:203 257:204 258:205 259:206 260:207 261:208 262:209 263:210 264:210 265:211 266:212 267:212 268:213 269:214 270:215 271:215\n",
            "I0905 15:33:46.769147 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True 261:True 262:True 263:True 264:True 265:True 266:True 267:True 268:True 269:True 270:True 271:True\n",
            "I0905 15:33:46.770664 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 1999 2029 2095 2106 1996 2691 3168 9491 2707 4772 1999 10289 8214 1029 102 2004 2012 2087 2060 5534 1010 10289 8214 1005 1055 2493 2448 1037 2193 1997 2739 2865 11730 1012 1996 3157 3076 1011 2448 11730 2421 2093 6399 1010 2119 1037 2557 1998 2547 2276 1010 1998 2195 7298 1998 9263 1012 5625 2004 1037 2028 1011 3931 3485 1999 2244 7326 1010 1996 24105 2932 2003 3843 3807 7058 1998 4447 2000 2022 1996 4587 7142 9234 4772 1999 1996 2142 2163 1012 1996 2060 2932 1010 1996 26536 17420 1010 2003 2207 3807 1037 2095 1998 7679 2006 3076 3906 1998 8266 1012 1996 8514 24803 2003 2405 6604 1012 1996 6399 2031 9671 4772 5426 1010 2007 1996 9718 2405 3679 1998 3701 7316 2118 1998 2060 2739 1010 1998 21121 2011 2493 2013 2119 10289 8214 1998 3002 2984 1005 1055 2267 1012 4406 24105 1998 1996 8514 1010 1996 9718 2003 2019 2981 4772 1998 2515 2025 2031 1037 4513 8619 2030 2151 8368 15709 2013 1996 2118 1012 1999 3055 1010 2043 2070 2493 3373 2008 1996 9718 2211 2000 2265 1037 4603 13827 1010 1037 4314 3780 1010 2691 3168 2001 2405 1012 10655 1010 1999 2494 1010 2043 2060 2493 3373 2008 1996 3259 3662 1037 4314 13827 1010 1996 4603 3259 3493 13631 2253 2046 2537 1012 4445 3259 2003 2405 2004 2411 2004 1996 9718 1025 2174 1010 2035 2093 2024 5500 2000 2035 2493 1012 2633 1010 1999 3500 2263 2019 8324 3485 2005 2576 2671 2470 1010 3458 4331 1010 2081 2049 2834 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.772333 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.773761 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.775332 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 181\n",
            "I0905 15:33:46.776679 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 181\n",
            "I0905 15:33:46.778221 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: 1987\n",
            "I0905 15:33:46.787130 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.788538 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034250\n",
            "I0905 15:33:46.790746 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 10\n",
            "I0905 15:33:46.792563 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.793923 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] where is the headquarters of the holy cross ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:33:46.795455 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:16 30:17 31:18 32:19 33:20 34:20 35:20 36:21 37:22 38:23 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:38 59:39 60:40 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:52 75:53 76:54 77:54 78:55 79:56 80:57 81:57 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:68 95:69 96:69 97:69 98:70 99:71 100:72 101:72 102:73 103:74 104:75 105:76 106:76 107:76 108:77 109:78 110:79 111:80 112:80 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:92 128:92 129:92 130:93 131:94 132:95 133:95 134:96 135:96 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:107 149:108 150:108 151:108 152:109 153:110 154:111 155:111\n",
            "I0905 15:33:46.796972 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0905 15:33:46.799145 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2073 2003 1996 4075 1997 1996 4151 2892 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.800635 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.802089 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.803228 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 33\n",
            "I0905 15:33:46.804471 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 33\n",
            "I0905 15:33:46.806415 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: rome\n",
            "I0905 15:33:46.816015 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.817282 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034251\n",
            "I0905 15:33:46.818239 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 11\n",
            "I0905 15:33:46.819064 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.820196 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is the most important seminary of the holy cross ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:33:46.821643 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:8 22:9 23:10 24:11 25:12 26:12 27:13 28:14 29:15 30:16 31:16 32:17 33:18 34:19 35:20 36:20 37:20 38:21 39:22 40:23 41:23 42:24 43:24 44:25 45:25 46:26 47:27 48:28 49:29 50:30 51:31 52:32 53:32 54:33 55:34 56:35 57:36 58:37 59:38 60:38 61:39 62:40 63:40 64:41 65:42 66:43 67:44 68:45 69:46 70:47 71:48 72:49 73:50 74:51 75:52 76:52 77:53 78:54 79:54 80:55 81:56 82:57 83:57 84:57 85:58 86:59 87:60 88:61 89:62 90:63 91:64 92:65 93:66 94:66 95:67 96:68 97:69 98:69 99:69 100:70 101:71 102:72 103:72 104:73 105:74 106:75 107:76 108:76 109:76 110:77 111:78 112:79 113:80 114:80 115:80 116:81 117:82 118:83 119:84 120:85 121:85 122:86 123:87 124:88 125:89 126:90 127:91 128:92 129:92 130:92 131:92 132:93 133:94 134:95 135:95 136:96 137:96 138:96 139:97 140:98 141:99 142:100 143:101 144:102 145:103 146:104 147:104 148:105 149:106 150:107 151:108 152:108 153:108 154:109 155:110 156:111 157:111\n",
            "I0905 15:33:46.823933 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True\n",
            "I0905 15:33:46.825378 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1996 2087 2590 8705 1997 1996 4151 2892 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.826562 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.828008 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.828989 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 42\n",
            "I0905 15:33:46.830272 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 44\n",
            "I0905 15:33:46.830932 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: more ##au seminary\n",
            "I0905 15:33:46.837045 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.838359 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034252\n",
            "I0905 15:33:46.839373 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 12\n",
            "I0905 15:33:46.840844 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.842172 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what is the oldest building in notre dame ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:33:46.843772 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:16 30:17 31:18 32:19 33:20 34:20 35:20 36:21 37:22 38:23 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:38 59:39 60:40 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:52 75:53 76:54 77:54 78:55 79:56 80:57 81:57 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:68 95:69 96:69 97:69 98:70 99:71 100:72 101:72 102:73 103:74 104:75 105:76 106:76 107:76 108:77 109:78 110:79 111:80 112:80 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:92 128:92 129:92 130:93 131:94 132:95 133:95 134:96 135:96 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:107 149:108 150:108 151:108 152:109 153:110 154:111 155:111\n",
            "I0905 15:33:46.845093 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0905 15:33:46.846735 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 2003 1996 4587 2311 1999 10289 8214 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.848143 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.849695 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.850977 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 59\n",
            "I0905 15:33:46.851940 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 60\n",
            "I0905 15:33:46.853192 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: old college\n",
            "I0905 15:33:46.862121 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.863581 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034253\n",
            "I0905 15:33:46.865541 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 13\n",
            "I0905 15:33:46.867250 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.868336 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] who lives in the fatima house in notre dame ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:33:46.870208 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:12 26:13 27:14 28:15 29:16 30:16 31:17 32:18 33:19 34:20 35:20 36:20 37:21 38:22 39:23 40:23 41:24 42:24 43:25 44:25 45:26 46:27 47:28 48:29 49:30 50:31 51:32 52:32 53:33 54:34 55:35 56:36 57:37 58:38 59:38 60:39 61:40 62:40 63:41 64:42 65:43 66:44 67:45 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:52 76:53 77:54 78:54 79:55 80:56 81:57 82:57 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:68 96:69 97:69 98:69 99:70 100:71 101:72 102:72 103:73 104:74 105:75 106:76 107:76 108:76 109:77 110:78 111:79 112:80 113:80 114:80 115:81 116:82 117:83 118:84 119:85 120:85 121:86 122:87 123:88 124:89 125:90 126:91 127:92 128:92 129:92 130:92 131:93 132:94 133:95 134:95 135:96 136:96 137:96 138:97 139:98 140:99 141:100 142:101 143:102 144:103 145:104 146:104 147:105 148:106 149:107 150:108 151:108 152:108 153:109 154:110 155:111 156:111\n",
            "I0905 15:33:46.871305 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True\n",
            "I0905 15:33:46.872792 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2040 3268 1999 1996 27596 2160 1999 10289 8214 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.874967 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.876394 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.877772 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 84\n",
            "I0905 15:33:46.878683 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 87\n",
            "I0905 15:33:46.879891 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: retired priests and brothers\n",
            "I0905 15:33:46.889044 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.890944 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034254\n",
            "I0905 15:33:46.892294 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 14\n",
            "I0905 15:33:46.893990 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.895069 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] what price has frederick bu ##ech ##ner created ? [SEP] the university is the major seat of the congregation of holy cross ( albeit not its official headquarters , which are in rome ) . its main seminary , more ##au seminary , is located on the campus across st . joseph lake from the main building . old college , the oldest building on campus and located near the shore of st . mary lake , houses undergraduate seminar ##ians . retired priests and brothers reside in fatima house ( a former retreat center ) , holy cross house , as well as col ##umb ##a hall near the gr ##otto . the university through the more ##au seminary has ties to theologian frederick bu ##ech ##ner . while not catholic , bu ##ech ##ner has praised writers from notre dame and more ##au seminary created a bu ##ech ##ner prize for preaching . [SEP]\n",
            "I0905 15:33:46.896431 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:12 25:13 26:14 27:15 28:16 29:16 30:17 31:18 32:19 33:20 34:20 35:20 36:21 37:22 38:23 39:23 40:24 41:24 42:25 43:25 44:26 45:27 46:28 47:29 48:30 49:31 50:32 51:32 52:33 53:34 54:35 55:36 56:37 57:38 58:38 59:39 60:40 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:52 75:53 76:54 77:54 78:55 79:56 80:57 81:57 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:68 95:69 96:69 97:69 98:70 99:71 100:72 101:72 102:73 103:74 104:75 105:76 106:76 107:76 108:77 109:78 110:79 111:80 112:80 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:92 128:92 129:92 130:93 131:94 132:95 133:95 134:96 135:96 136:96 137:97 138:98 139:99 140:100 141:101 142:102 143:103 144:104 145:104 146:105 147:106 148:107 149:108 150:108 151:108 152:109 153:110 154:111 155:111\n",
            "I0905 15:33:46.899290 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0905 15:33:46.900486 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2054 3976 2038 5406 20934 15937 3678 2580 1029 102 1996 2118 2003 1996 2350 2835 1997 1996 7769 1997 4151 2892 1006 12167 2025 2049 2880 4075 1010 2029 2024 1999 4199 1007 1012 2049 2364 8705 1010 2062 4887 8705 1010 2003 2284 2006 1996 3721 2408 2358 1012 3312 2697 2013 1996 2364 2311 1012 2214 2267 1010 1996 4587 2311 2006 3721 1998 2284 2379 1996 5370 1997 2358 1012 2984 2697 1010 3506 8324 18014 7066 1012 3394 8656 1998 3428 13960 1999 27596 2160 1006 1037 2280 7822 2415 1007 1010 4151 2892 2160 1010 2004 2092 2004 8902 25438 2050 2534 2379 1996 24665 23052 1012 1996 2118 2083 1996 2062 4887 8705 2038 7208 2000 17200 5406 20934 15937 3678 1012 2096 2025 3234 1010 20934 15937 3678 2038 5868 4898 2013 10289 8214 1998 2062 4887 8705 2580 1037 20934 15937 3678 3396 2005 17979 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.902401 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.903896 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.904870 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 149\n",
            "I0905 15:33:46.907180 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 154\n",
            "I0905 15:33:46.908730 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: bu ##ech ##ner prize for preaching\n",
            "I0905 15:33:46.917987 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.919822 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034255\n",
            "I0905 15:33:46.921097 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 15\n",
            "I0905 15:33:46.922474 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.923849 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] how many bachelor degrees are offered at notre dame school of engineering ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:33:46.924918 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 15:0 16:1 17:2 18:3 19:4 20:5 21:6 22:7 23:7 24:8 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:16 34:17 35:18 36:19 37:20 38:21 39:22 40:23 41:24 42:25 43:26 44:26 45:27 46:28 47:29 48:29 49:30 50:31 51:32 52:33 53:33 54:34 55:34 56:34 57:35 58:36 59:36 60:36 61:36 62:36 63:36 64:36 65:37 66:38 67:39 68:39 69:40 70:41 71:42 72:43 73:44 74:45 75:46 76:47 77:48 78:49 79:49 80:50 81:51 82:52 83:52 84:52 85:52 86:53 87:53 88:54 89:55 90:56 91:57 92:58 93:58 94:59 95:60 96:61 97:62 98:62 99:63 100:64 101:65 102:66 103:67 104:68 105:69 106:69 107:69 108:69 109:70 110:71 111:71 112:72 113:72 114:73 115:74 116:75 117:76 118:76 119:76 120:77 121:78 122:79 123:80 124:81 125:82 126:83 127:84 128:85 129:86 130:87 131:88 132:89 133:90 134:91 135:92 136:92 137:92 138:92 139:93 140:94 141:95 142:96 143:97 144:98 145:98 146:98 147:99 148:99 149:100 150:100\n",
            "I0905 15:33:46.926785 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True\n",
            "I0905 15:33:46.928239 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2129 2116 5065 5445 2024 3253 2012 10289 8214 2082 1997 3330 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.931061 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.932951 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.934510 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 104\n",
            "I0905 15:33:46.935787 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 104\n",
            "I0905 15:33:46.936690 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: eight\n",
            "I0905 15:33:46.945857 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.948230 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034256\n",
            "I0905 15:33:46.949577 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 16\n",
            "I0905 15:33:46.950564 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.951963 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] in which year was the notre dame technical school founded ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:33:46.953363 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 13:0 14:1 15:2 16:3 17:4 18:5 19:6 20:7 21:7 22:8 23:8 24:9 25:10 26:11 27:12 28:13 29:14 30:15 31:16 32:17 33:18 34:19 35:20 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:27 44:28 45:29 46:29 47:30 48:31 49:32 50:33 51:33 52:34 53:34 54:34 55:35 56:36 57:36 58:36 59:36 60:36 61:36 62:36 63:37 64:38 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:47 75:48 76:49 77:49 78:50 79:51 80:52 81:52 82:52 83:52 84:53 85:53 86:54 87:55 88:56 89:57 90:58 91:58 92:59 93:60 94:61 95:62 96:62 97:63 98:64 99:65 100:66 101:67 102:68 103:69 104:69 105:69 106:69 107:70 108:71 109:71 110:72 111:72 112:73 113:74 114:75 115:76 116:76 117:76 118:77 119:78 120:79 121:80 122:81 123:82 124:83 125:84 126:85 127:86 128:87 129:88 130:89 131:90 132:91 133:92 134:92 135:92 136:92 137:93 138:94 139:95 140:96 141:97 142:98 143:98 144:98 145:99 146:99 147:100 148:100\n",
            "I0905 15:33:46.954472 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True\n",
            "I0905 15:33:46.957095 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 1999 2029 2095 2001 1996 10289 8214 4087 2082 2631 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.958642 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.960051 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.961974 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 20\n",
            "I0905 15:33:46.963204 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 20\n",
            "I0905 15:33:46.964252 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: 1920\n",
            "I0905 15:33:46.974403 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:46.976093 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034257\n",
            "I0905 15:33:46.977150 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 17\n",
            "I0905 15:33:46.979237 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:46.980629 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] before the establishment of the school of engineering , a similar study was conducted at notre dame de paris . [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:33:46.981995 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 22:0 23:1 24:2 25:3 26:4 27:5 28:6 29:7 30:7 31:8 32:8 33:9 34:10 35:11 36:12 37:13 38:14 39:15 40:16 41:17 42:18 43:19 44:20 45:21 46:22 47:23 48:24 49:25 50:26 51:26 52:27 53:28 54:29 55:29 56:30 57:31 58:32 59:33 60:33 61:34 62:34 63:34 64:35 65:36 66:36 67:36 68:36 69:36 70:36 71:36 72:37 73:38 74:39 75:39 76:40 77:41 78:42 79:43 80:44 81:45 82:46 83:47 84:48 85:49 86:49 87:50 88:51 89:52 90:52 91:52 92:52 93:53 94:53 95:54 96:55 97:56 98:57 99:58 100:58 101:59 102:60 103:61 104:62 105:62 106:63 107:64 108:65 109:66 110:67 111:68 112:69 113:69 114:69 115:69 116:70 117:71 118:71 119:72 120:72 121:73 122:74 123:75 124:76 125:76 126:76 127:77 128:78 129:79 130:80 131:81 132:82 133:83 134:84 135:85 136:86 137:87 138:88 139:89 140:90 141:91 142:92 143:92 144:92 145:92 146:93 147:94 148:95 149:96 150:97 151:98 152:98 153:98 154:99 155:99 156:100 157:100\n",
            "I0905 15:33:46.983076 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True\n",
            "I0905 15:33:46.984506 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2077 1996 5069 1997 1996 2082 1997 3330 1010 1037 2714 2817 2001 4146 2012 10289 8214 2139 3000 1012 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.986660 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.988116 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:46.989150 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 44\n",
            "I0905 15:33:46.990443 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 47\n",
            "I0905 15:33:46.992632 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the college of science\n",
            "I0905 15:33:47.002902 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:47.004692 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034258\n",
            "I0905 15:33:47.006366 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 18\n",
            "I0905 15:33:47.007933 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:47.009298 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] how many departments are there in the st ##ins ##on - re ##mic ##k engineering hall ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:33:47.010680 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:7 27:7 28:8 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:27 50:28 51:29 52:29 53:30 54:31 55:32 56:33 57:33 58:34 59:34 60:34 61:35 62:36 63:36 64:36 65:36 66:36 67:36 68:36 69:37 70:38 71:39 72:39 73:40 74:41 75:42 76:43 77:44 78:45 79:46 80:47 81:48 82:49 83:49 84:50 85:51 86:52 87:52 88:52 89:52 90:53 91:53 92:54 93:55 94:56 95:57 96:58 97:58 98:59 99:60 100:61 101:62 102:62 103:63 104:64 105:65 106:66 107:67 108:68 109:69 110:69 111:69 112:69 113:70 114:71 115:71 116:72 117:72 118:73 119:74 120:75 121:76 122:76 123:76 124:77 125:78 126:79 127:80 128:81 129:82 130:83 131:84 132:85 133:86 134:87 135:88 136:89 137:90 138:91 139:92 140:92 141:92 142:92 143:93 144:94 145:95 146:96 147:97 148:98 149:98 150:98 151:99 152:99 153:100 154:100\n",
            "I0905 15:33:47.011798 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True\n",
            "I0905 15:33:47.013256 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 2129 2116 7640 2024 2045 1999 1996 2358 7076 2239 1011 2128 7712 2243 3330 2534 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:47.015396 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:47.016959 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:47.018337 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 74\n",
            "I0905 15:33:47.020106 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 74\n",
            "I0905 15:33:47.021135 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: five\n",
            "I0905 15:33:47.031501 140003207100288 <ipython-input-80-1d083eed5fec>:283] *** Example ***\n",
            "I0905 15:33:47.034141 140003207100288 <ipython-input-80-1d083eed5fec>:284] unique_id: 1000034259\n",
            "I0905 15:33:47.035576 140003207100288 <ipython-input-80-1d083eed5fec>:285] example_index: 19\n",
            "I0905 15:33:47.036617 140003207100288 <ipython-input-80-1d083eed5fec>:286] doc_span_index: 0\n",
            "I0905 15:33:47.038259 140003207100288 <ipython-input-80-1d083eed5fec>:288] tokens: [CLS] the academy of sciences started offering civil engineering courses . when did it start in notre dame ? [SEP] the college of engineering was established in 1920 , however , early courses in civil and mechanical engineering were a part of the college of science since the 1870s . today the college , housed in the fitzpatrick , cu ##shing , and st ##ins ##on - re ##mic ##k halls of engineering , includes five departments of study – aerospace and mechanical engineering , chemical and bio ##mo ##le ##cular engineering , civil engineering and geological sciences , computer science and engineering , and electrical engineering – with eight b . s . degrees offered . additionally , the college offers five - year dual degree programs with the colleges of arts and letters and of business awarding additional b . a . and master of business administration ( mba ) degrees , respectively . [SEP]\n",
            "I0905 15:33:47.040382 140003207100288 <ipython-input-80-1d083eed5fec>:290] token_to_orig_map: 20:0 21:1 22:2 23:3 24:4 25:5 26:6 27:7 28:7 29:8 30:8 31:9 32:10 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:23 46:24 47:25 48:26 49:26 50:27 51:28 52:29 53:29 54:30 55:31 56:32 57:33 58:33 59:34 60:34 61:34 62:35 63:36 64:36 65:36 66:36 67:36 68:36 69:36 70:37 71:38 72:39 73:39 74:40 75:41 76:42 77:43 78:44 79:45 80:46 81:47 82:48 83:49 84:49 85:50 86:51 87:52 88:52 89:52 90:52 91:53 92:53 93:54 94:55 95:56 96:57 97:58 98:58 99:59 100:60 101:61 102:62 103:62 104:63 105:64 106:65 107:66 108:67 109:68 110:69 111:69 112:69 113:69 114:70 115:71 116:71 117:72 118:72 119:73 120:74 121:75 122:76 123:76 124:76 125:77 126:78 127:79 128:80 129:81 130:82 131:83 132:84 133:85 134:86 135:87 136:88 137:89 138:90 139:91 140:92 141:92 142:92 143:92 144:93 145:94 146:95 147:96 148:97 149:98 150:98 151:98 152:99 153:99 154:100 155:100\n",
            "I0905 15:33:47.041879 140003207100288 <ipython-input-80-1d083eed5fec>:292] token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n",
            "I0905 15:33:47.043625 140003207100288 <ipython-input-80-1d083eed5fec>:294] input_ids: 101 1996 2914 1997 4163 2318 5378 2942 3330 5352 1012 2043 2106 2009 2707 1999 10289 8214 1029 102 1996 2267 1997 3330 2001 2511 1999 4444 1010 2174 1010 2220 5352 1999 2942 1998 6228 3330 2020 1037 2112 1997 1996 2267 1997 2671 2144 1996 14896 1012 2651 1996 2267 1010 7431 1999 1996 26249 1010 12731 12227 1010 1998 2358 7076 2239 1011 2128 7712 2243 9873 1997 3330 1010 2950 2274 7640 1997 2817 1516 13395 1998 6228 3330 1010 5072 1998 16012 5302 2571 15431 3330 1010 2942 3330 1998 9843 4163 1010 3274 2671 1998 3330 1010 1998 5992 3330 1516 2007 2809 1038 1012 1055 1012 5445 3253 1012 5678 1010 1996 2267 4107 2274 1011 2095 7037 3014 3454 2007 1996 6667 1997 2840 1998 4144 1998 1997 2449 21467 3176 1038 1012 1037 1012 1998 3040 1997 2449 3447 1006 15038 1007 5445 1010 4414 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:47.045072 140003207100288 <ipython-input-80-1d083eed5fec>:296] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:47.047264 140003207100288 <ipython-input-80-1d083eed5fec>:298] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:33:47.048579 140003207100288 <ipython-input-80-1d083eed5fec>:303] start_position: 47\n",
            "I0905 15:33:47.049471 140003207100288 <ipython-input-80-1d083eed5fec>:304] end_position: 48\n",
            "I0905 15:33:47.050918 140003207100288 <ipython-input-80-1d083eed5fec>:306] answer: the 1870s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "34812\n",
            "34812\n",
            "Processed All Features!!!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it5e-gIOCQGl",
        "colab_type": "code",
        "outputId": "54a21a43-7aa6-478a-dd38-8bf01c5fee51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0905 15:36:08.828463 140003207100288 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.94.95.10:8470) for TPU system metadata.\n",
            "I0905 15:36:08.842790 140003207100288 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0905 15:36:08.845844 140003207100288 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0905 15:36:08.846923 140003207100288 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0905 15:36:08.847861 140003207100288 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0905 15:36:08.849061 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6396140126797673023)\n",
            "I0905 15:36:08.849969 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15519625609306575865)\n",
            "I0905 15:36:08.850873 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5647880652992250092)\n",
            "I0905 15:36:08.852032 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10116309137384218122)\n",
            "I0905 15:36:08.852950 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 626848651240203477)\n",
            "I0905 15:36:08.853883 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7897455746696599298)\n",
            "I0905 15:36:08.854772 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1647090969102948083)\n",
            "I0905 15:36:08.855637 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 722087878004646180)\n",
            "I0905 15:36:08.856463 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10338868663690472259)\n",
            "I0905 15:36:08.857320 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17057546330107334435)\n",
            "I0905 15:36:08.858172 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4404943996043416504)\n",
            "I0905 15:36:08.881366 140003207100288 estimator.py:1145] Calling model_fn.\n",
            "I0905 15:36:08.982680 140003207100288 <ipython-input-81-d426c81925ed>:89] *** Features ***\n",
            "I0905 15:36:08.983733 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = end_positions, shape = (2,)\n",
            "I0905 15:36:08.984671 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = end_positions_1, shape = (2,)\n",
            "I0905 15:36:08.985581 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = input_ids, shape = (2, 386)\n",
            "I0905 15:36:08.986447 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = input_mask, shape = (2, 386)\n",
            "I0905 15:36:08.987351 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = segment_ids, shape = (2, 386)\n",
            "I0905 15:36:08.988370 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = start_positions, shape = (2,)\n",
            "I0905 15:36:08.989368 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = start_positions_1, shape = (2,)\n",
            "I0905 15:36:08.990334 140003207100288 <ipython-input-81-d426c81925ed>:91]   name = unique_ids, shape = (2,)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Example: {'end_positions': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=() dtype=int64>, 'end_positions_1': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=() dtype=int64>, 'input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(386,) dtype=int64>, 'input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=(386,) dtype=int64>, 'segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:4' shape=(386,) dtype=int64>, 'start_positions': <tf.Tensor 'ParseSingleExample/ParseSingleExample:5' shape=() dtype=int64>, 'start_positions_1': <tf.Tensor 'ParseSingleExample/ParseSingleExample:6' shape=() dtype=int64>, 'unique_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:7' shape=() dtype=int64>}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0905 15:36:16.343756 140003207100288 <ipython-input-81-d426c81925ed>:125] **** Trainable Variables ****\n",
            "I0905 15:36:16.347114 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/embeddings/word_embeddings:0, shape = (30522, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.348604 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.349848 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/embeddings/position_embeddings:0, shape = (512, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.351167 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/embeddings/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.352478 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/embeddings/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.353683 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.354970 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.356183 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.357416 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.358713 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.359846 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.360941 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.362150 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.363370 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.364349 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.365694 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.366896 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.368059 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.369190 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.370277 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.371491 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.372540 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.373729 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.374968 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.376073 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.377256 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.378375 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.379397 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.380496 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.381605 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.382666 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.383765 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.384880 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.385917 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.387051 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.388163 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.389188 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.390282 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.391400 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.392436 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.393560 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.394672 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.395784 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.396902 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.398011 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.399043 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.400159 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.401274 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.402316 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.403423 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.404598 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.405683 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.406899 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.407998 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.409042 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.410137 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.411302 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.412335 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.413409 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.414546 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.415611 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.416718 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.417965 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.419970 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.421077 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.422143 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.423274 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.424398 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.425429 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.426554 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.427685 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.428808 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.429894 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.431001 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.432122 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.433166 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.434280 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.435410 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.436478 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.437722 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.438881 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.439938 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.441073 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.442267 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.443369 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.444461 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.445609 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.446717 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.447846 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.448892 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.450029 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.451088 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.452311 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.453413 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.454455 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.455609 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.456716 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.457780 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.458852 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.459962 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.461027 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.462292 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.463384 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.464465 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.465625 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.466727 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.467805 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.468871 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.469971 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.471034 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.472114 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.473209 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.474264 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.475372 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.476525 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.477589 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.478505 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.479799 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.480915 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.481937 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.483042 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.484139 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.485182 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.486267 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.487400 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.488454 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.489651 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.490746 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.491818 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.492897 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.494015 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.495076 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.496185 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.497285 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.498365 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.499462 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.500610 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.501652 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.502759 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.503854 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.504951 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.506043 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.507349 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.508384 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.509476 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.510618 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.511661 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.512771 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.513901 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.514945 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.516061 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.517189 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.518235 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.519321 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.520426 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.521473 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.522628 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.523729 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.524775 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.525872 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.526983 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.528013 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.529100 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.530226 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.531266 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.532328 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.533399 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.534490 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.535646 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.536764 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.537812 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.538928 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.540029 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.541056 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.542150 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.543264 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.544353 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.545456 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.546640 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.547687 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.548807 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.549941 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.551057 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.552671 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.554155 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.555238 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.556366 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.557391 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.558460 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.559545 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.561090 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.562114 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.563640 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.564710 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.565833 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.566854 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.568559 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.569460 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.570764 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.571841 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.572969 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.574082 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.575124 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.576079 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.577147 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.578180 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.579770 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.580835 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.581948 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.582970 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.584010 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.585044 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.586122 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_12/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.587234 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.588401 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.589424 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.590535 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.591600 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.592639 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.593709 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.594783 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.595843 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.596918 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.598446 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.599709 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.600831 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.601941 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.603028 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.604098 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_13/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.605189 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.606258 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.607371 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.608479 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.609586 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.610658 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.611794 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.612887 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.613947 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.615293 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.617635 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.618797 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.619763 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.621327 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.622389 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.623451 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_14/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.624475 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.625554 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.626590 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.627660 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.629158 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.630477 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.631571 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.632634 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.633938 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.634999 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.636240 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.637322 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.638403 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.639545 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.640613 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.641702 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_15/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.642800 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.643867 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.644996 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.646107 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.647233 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.648291 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.649401 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.650536 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.651622 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.652738 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.653855 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.654937 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.656076 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.657190 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.658259 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.659358 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_16/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.660483 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.661554 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.662681 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.663816 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.664861 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.665977 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.667115 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.668151 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.669244 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.670566 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.671648 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.672796 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.673920 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.674990 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.676129 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.677249 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_17/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.678298 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.679406 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.680563 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.681622 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.682729 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.683852 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.684893 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.685973 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.687089 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.688133 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.689228 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.690412 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.691484 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.692584 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.693711 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.694771 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_18/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.695971 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.696861 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.698440 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.699979 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.701231 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.702357 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.703469 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.704556 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.705670 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.706981 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.708156 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.709280 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.710312 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.711418 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.712560 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.713629 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_19/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.714717 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.715833 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.716906 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.718010 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.719117 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.720211 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.721354 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.722227 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.724044 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.725076 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.726147 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.727234 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.728257 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.729329 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.730400 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.731495 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_20/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.732813 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.734019 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.735335 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.737026 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.738318 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.739966 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.741253 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.742535 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.744034 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.745287 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.746351 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.747712 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.748740 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.750027 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.751780 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.753098 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_21/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.754313 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.755637 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.756861 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.757881 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.759176 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.760822 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.762375 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.763429 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.764707 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.766434 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.767763 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.769387 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.770939 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.772205 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.773563 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.774848 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_22/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.775685 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.777721 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.779023 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.780038 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.781226 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.782503 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.783620 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.785079 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.786445 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.787503 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.788409 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.789706 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.790749 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.791949 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.793043 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.794138 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/encoder/layer_23/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.795272 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/pooler/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.796429 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = bert/pooler/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:36:16.797460 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = cls/squad/output_weights:0, shape = (2, 1024)\n",
            "I0905 15:36:16.798557 140003207100288 <ipython-input-81-d426c81925ed>:131]   name = cls/squad/output_bias:0, shape = (2,)\n",
            "I0905 15:36:45.213852 140003207100288 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0905 15:36:45.901229 140003207100288 estimator.py:1147] Done calling model_fn.\n",
            "I0905 15:36:45.902641 140003207100288 tpu_estimator.py:499] TPU job name worker\n",
            "I0905 15:36:50.241163 140003207100288 monitored_session.py:240] Graph was finalized.\n",
            "I0905 15:37:10.425988 140003207100288 session_manager.py:500] Running local_init_op.\n",
            "I0905 15:37:12.499668 140003207100288 session_manager.py:502] Done running local_init_op.\n",
            "I0905 15:37:31.827390 140003207100288 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1/model.ckpt.\n",
            "I0905 15:38:41.757926 140003207100288 util.py:98] Initialized dataset iterators in 1 seconds\n",
            "I0905 15:38:41.759366 140003207100288 session_support.py:332] Installing graceful shutdown hook.\n",
            "I0905 15:38:41.767685 140003207100288 session_support.py:82] Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "I0905 15:38:41.771345 140003207100288 session_support.py:105] Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "I0905 15:38:41.776951 140003207100288 tpu_estimator.py:557] Init TPU system\n",
            "I0905 15:38:49.923220 140003207100288 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0905 15:38:49.925018 140000912295680 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0905 15:38:49.926245 140000920688384 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0905 15:38:51.820384 140003207100288 tpu_estimator.py:590] Enqueue next (500) batch(es) of data to infeed.\n",
            "I0905 15:38:51.821586 140003207100288 tpu_estimator.py:594] Dequeue next (500) batch(es) of data from outfeed.\n",
            "I0905 15:40:15.579478 140000920688384 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0905 15:41:15.756312 140000920688384 tpu_estimator.py:275] Outfeed finished for iteration (0, 258)\n",
            "I0905 15:42:14.052206 140003207100288 basic_session_run_hooks.py:606] Saving checkpoints for 500 into gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1/model.ckpt.\n",
            "I0905 15:43:38.561847 140003207100288 basic_session_run_hooks.py:262] loss = 0.70369923, step = 500\n",
            "I0905 15:43:38.565736 140003207100288 tpu_estimator.py:590] Enqueue next (35) batch(es) of data to infeed.\n",
            "I0905 15:43:38.567580 140003207100288 tpu_estimator.py:594] Dequeue next (35) batch(es) of data from outfeed.\n",
            "I0905 15:43:56.244824 140000920688384 tpu_estimator.py:275] Outfeed finished for iteration (1, 0)\n",
            "I0905 15:44:06.127877 140003207100288 basic_session_run_hooks.py:260] loss = 0.005315599, step = 535 (27.566 sec)\n",
            "I0905 15:44:06.130694 140003207100288 tpu_estimator.py:2159] global_step/sec: 1.26969\n",
            "I0905 15:44:06.132983 140003207100288 tpu_estimator.py:2160] examples/sec: 20.315\n",
            "I0905 15:44:06.137625 140003207100288 basic_session_run_hooks.py:606] Saving checkpoints for 535 into gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1/model.ckpt.\n",
            "I0905 15:45:15.884474 140003207100288 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0905 15:45:15.885836 140003207100288 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0905 15:45:15.891397 140000912295680 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0905 15:45:15.892642 140000912295680 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0905 15:45:15.897012 140003207100288 error_handling.py:96] infeed marked as finished\n",
            "I0905 15:45:15.899606 140003207100288 tpu_estimator.py:602] Stop output thread controller\n",
            "I0905 15:45:15.900966 140003207100288 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0905 15:45:15.902277 140000920688384 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0905 15:45:15.903117 140000920688384 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0905 15:45:15.904128 140003207100288 error_handling.py:96] outfeed marked as finished\n",
            "I0905 15:45:15.905816 140003207100288 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0905 15:45:19.339393 140003207100288 estimator.py:368] Loss for final step: 0.005315599.\n",
            "I0905 15:45:19.340592 140003207100288 error_handling.py:96] training_loop marked as finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.tpu.tpu_estimator.TPUEstimator at 0x7f54975bf898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNj_usKxgzlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PARAPHRASED_FILE ='gs://bert_bucket_new/bert/files/dev-v1.1_paraphrased.json'\n",
        "eval_examples_CD_small = read_squad_examples(\n",
        "    input_file=PARAPHRASED_FILE, is_training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA_U1syztMn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EVAL_FILE ='gs://bert_bucket_new/bert/files/adversarial.json'\n",
        "eval_examples_adv = read_squad_examples(\n",
        "    input_file=EVAL_FILE, is_training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q954I25YL6A",
        "colab_type": "code",
        "outputId": "ea22e2ed-8efa-46c6-9664-ee373e95c06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "estimator = get_estimator(TRAIN_BATCH_SIZE, LEARNING_RATE, NUM_TRAIN_EPOCHS, WARMUP_PROPORTION, num_train_steps)\n",
        "eval_input_fn, eval_features, all_tokens = get_eval_input_fn(eval_examples_adv)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0905 15:48:27.192561 140003207100288 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5495ae5ea0>) includes params argument, but params are not passed to Estimator.\n",
            "I0905 15:48:27.195663 140003207100288 estimator.py:209] Using config: {'_model_dir': 'gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.94.95.10:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f547c378898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.94.95.10:8470', '_evaluation_master': 'grpc://10.94.95.10:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu_cluster_resolver.TPUClusterResolver object at 0x7f548885b898>}\n",
            "I0905 15:48:27.197024 140003207100288 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "I0905 15:48:27.209723 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.211451 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000000\n",
            "I0905 15:48:27.214605 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 0\n",
            "I0905 15:48:27.215726 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.217258 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] where did super bowl 50 take place ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0905 15:48:27.218502 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123\n",
            "I0905 15:48:27.219763 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True\n",
            "I0905 15:48:27.221011 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2073 2106 3565 4605 2753 2202 2173 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.222245 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.223470 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.232366 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.233497 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000001\n",
            "I0905 15:48:27.234617 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 1\n",
            "I0905 15:48:27.235897 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.237016 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what day was the super bowl played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0905 15:48:27.238199 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123\n",
            "I0905 15:48:27.239649 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True\n",
            "I0905 15:48:27.240870 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2154 2001 1996 3565 4605 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.242302 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.243556 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.251557 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.252685 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000002\n",
            "I0905 15:48:27.253755 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 2\n",
            "I0905 15:48:27.254828 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.255972 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what venue did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0905 15:48:27.257428 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123\n",
            "I0905 15:48:27.258625 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0905 15:48:27.259838 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 6891 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.261529 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.262773 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.270975 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.272238 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000003\n",
            "I0905 15:48:27.273320 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 3\n",
            "I0905 15:48:27.274631 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.275764 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what year did the denver broncos secure a super bowl title for the third time ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0905 15:48:27.276948 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:35 60:35 61:36 62:37 63:38 64:39 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:80 118:81 119:82 120:83 121:83 122:83 123:84 124:84 125:85 126:86 127:87 128:88 129:89 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:110 156:111 157:112 158:112 159:112 160:112 161:113 162:114 163:115 164:116 165:117 166:118 167:119 168:120 169:121 170:122 171:122 172:122 173:123 174:123\n",
            "I0905 15:48:27.278132 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0905 15:48:27.279386 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2095 2106 1996 7573 14169 5851 1037 3565 4605 2516 2005 1996 2353 2051 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.280641 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.281828 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.290063 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.291138 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000004\n",
            "I0905 15:48:27.292137 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 4\n",
            "I0905 15:48:27.293390 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.294614 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what was the final score of super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0905 15:48:27.295790 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123\n",
            "I0905 15:48:27.297172 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True\n",
            "I0905 15:48:27.298395 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2001 1996 2345 3556 1997 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.299905 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.301130 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.309449 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.310617 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000005\n",
            "I0905 15:48:27.311821 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 5\n",
            "I0905 15:48:27.312947 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.314084 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] who won super bowl 50 ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . [SEP]\n",
            "I0905 15:48:27.315634 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 8:0 9:1 10:2 11:3 12:4 13:5 14:6 15:7 16:8 17:9 18:10 19:11 20:12 21:13 22:14 23:15 24:16 25:17 26:17 27:17 28:18 29:19 30:20 31:21 32:21 33:22 34:23 35:24 36:25 37:26 38:26 39:26 40:27 41:28 42:29 43:30 44:31 45:32 46:33 47:34 48:35 49:35 50:35 51:36 52:37 53:38 54:39 55:39 56:39 57:40 58:41 59:42 60:43 61:44 62:45 63:46 64:46 65:47 66:48 67:49 68:50 69:51 70:52 71:53 72:53 73:54 74:54 75:55 76:56 77:56 78:56 79:57 80:58 81:59 82:60 83:61 84:62 85:63 86:64 87:65 88:66 89:66 90:67 91:67 92:68 93:69 94:70 95:71 96:72 97:73 98:74 99:74 100:75 101:76 102:77 103:78 104:79 105:79 106:80 107:80 108:81 109:82 110:83 111:83 112:83 113:84 114:84 115:85 116:86 117:87 118:88 119:89 120:89 121:90 122:91 123:92 124:93 125:94 126:95 127:96 128:97 129:98 130:99 131:100 132:100 133:100 134:101 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:110 146:111 147:112 148:112 149:112 150:112 151:113 152:114 153:115 154:116 155:117 156:118 157:119 158:120 159:121 160:122 161:122 162:122 163:123 164:123\n",
            "I0905 15:48:27.316803 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 8:True 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True\n",
            "I0905 15:48:27.318022 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2040 2180 3565 4605 2753 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.319267 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.320551 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.329612 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.330723 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000006\n",
            "I0905 15:48:27.331967 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 6\n",
            "I0905 15:48:27.332983 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.334068 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] where did super bowl 50 take place ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the champ bowl 40 took place in chicago . [SEP]\n",
            "I0905 15:48:27.335456 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123 167:124 168:125 169:126 170:127 171:128 172:129 173:130 174:131 175:131\n",
            "I0905 15:48:27.336643 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True\n",
            "I0905 15:48:27.338041 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2073 2106 3565 4605 2753 2202 2173 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 24782 4605 2871 2165 2173 1999 3190 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.339256 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.340507 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.348691 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.349803 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000007\n",
            "I0905 15:48:27.350831 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 7\n",
            "I0905 15:48:27.351846 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.353239 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] where did super bowl 50 take place ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . champ bowl 40 took place in chicago . [SEP]\n",
            "I0905 15:48:27.354380 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 10:0 11:1 12:2 13:3 14:4 15:5 16:6 17:7 18:8 19:9 20:10 21:11 22:12 23:13 24:14 25:15 26:16 27:17 28:17 29:17 30:18 31:19 32:20 33:21 34:21 35:22 36:23 37:24 38:25 39:26 40:26 41:26 42:27 43:28 44:29 45:30 46:31 47:32 48:33 49:34 50:35 51:35 52:35 53:36 54:37 55:38 56:39 57:39 58:39 59:40 60:41 61:42 62:43 63:44 64:45 65:46 66:46 67:47 68:48 69:49 70:50 71:51 72:52 73:53 74:53 75:54 76:54 77:55 78:56 79:56 80:56 81:57 82:58 83:59 84:60 85:61 86:62 87:63 88:64 89:65 90:66 91:66 92:67 93:67 94:68 95:69 96:70 97:71 98:72 99:73 100:74 101:74 102:75 103:76 104:77 105:78 106:79 107:79 108:80 109:80 110:81 111:82 112:83 113:83 114:83 115:84 116:84 117:85 118:86 119:87 120:88 121:89 122:89 123:90 124:91 125:92 126:93 127:94 128:95 129:96 130:97 131:98 132:99 133:100 134:100 135:100 136:101 137:101 138:102 139:103 140:104 141:105 142:106 143:107 144:108 145:109 146:110 147:110 148:111 149:112 150:112 151:112 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:122 164:122 165:123 166:123 167:124 168:125 169:126 170:127 171:128 172:129 173:130 174:130\n",
            "I0905 15:48:27.355571 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "I0905 15:48:27.357018 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2073 2106 3565 4605 2753 2202 2173 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 24782 4605 2871 2165 2173 1999 3190 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.358218 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.359842 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.368134 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.369430 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000008\n",
            "I0905 15:48:27.370479 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 8\n",
            "I0905 15:48:27.371498 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.372649 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what day was the super bowl played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the champ bowl was played on august 18 , 1991 . [SEP]\n",
            "I0905 15:48:27.373808 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123 168:124 169:125 170:126 171:127 172:128 173:129 174:130 175:131 176:131 177:132 178:132\n",
            "I0905 15:48:27.374989 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True\n",
            "I0905 15:48:27.376253 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2154 2001 1996 3565 4605 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 24782 4605 2001 2209 2006 2257 2324 1010 2889 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.377810 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.379047 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.387732 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.388822 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000009\n",
            "I0905 15:48:27.389811 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 9\n",
            "I0905 15:48:27.391100 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.392279 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what day was the super bowl played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the champ bowl was played on the day of easter . [SEP]\n",
            "I0905 15:48:27.393847 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123 168:124 169:125 170:126 171:127 172:128 173:129 174:130 175:131 176:132 177:133 178:133\n",
            "I0905 15:48:27.395051 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True\n",
            "I0905 15:48:27.396340 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2154 2001 1996 3565 4605 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 24782 4605 2001 2209 2006 1996 2154 1997 10957 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.397763 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.399027 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.407343 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.408781 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000010\n",
            "I0905 15:48:27.409834 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 10\n",
            "I0905 15:48:27.410902 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.412077 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what day was the super bowl played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the champ bowl was played on august 18th , 1991 . [SEP]\n",
            "I0905 15:48:27.413609 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123 168:124 169:125 170:126 171:127 172:128 173:129 174:130 175:131 176:131 177:131 178:131\n",
            "I0905 15:48:27.414626 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True\n",
            "I0905 15:48:27.415776 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2154 2001 1996 3565 4605 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 24782 4605 2001 2209 2006 2257 4985 1010 2889 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.417711 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.419778 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.428747 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.430115 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000011\n",
            "I0905 15:48:27.431149 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 11\n",
            "I0905 15:48:27.432222 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.433613 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what day was the super bowl played on ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the champ bowl was played on the day of august 18 , 1991 . [SEP]\n",
            "I0905 15:48:27.435411 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 11:0 12:1 13:2 14:3 15:4 16:5 17:6 18:7 19:8 20:9 21:10 22:11 23:12 24:13 25:14 26:15 27:16 28:17 29:17 30:17 31:18 32:19 33:20 34:21 35:21 36:22 37:23 38:24 39:25 40:26 41:26 42:26 43:27 44:28 45:29 46:30 47:31 48:32 49:33 50:34 51:35 52:35 53:35 54:36 55:37 56:38 57:39 58:39 59:39 60:40 61:41 62:42 63:43 64:44 65:45 66:46 67:46 68:47 69:48 70:49 71:50 72:51 73:52 74:53 75:53 76:54 77:54 78:55 79:56 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:66 93:67 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:76 105:77 106:78 107:79 108:79 109:80 110:80 111:81 112:82 113:83 114:83 115:83 116:84 117:84 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:100 136:100 137:101 138:101 139:102 140:103 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:110 149:111 150:112 151:112 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:119 161:120 162:121 163:122 164:122 165:122 166:123 167:123 168:124 169:125 170:126 171:127 172:128 173:129 174:130 175:131 176:132 177:133 178:134 179:134 180:135 181:135\n",
            "I0905 15:48:27.436870 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True\n",
            "I0905 15:48:27.438144 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2154 2001 1996 3565 4605 2209 2006 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 24782 4605 2001 2209 2006 1996 2154 1997 2257 2324 1010 2889 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.439975 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.441717 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.450279 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.451734 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000012\n",
            "I0905 15:48:27.453323 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 12\n",
            "I0905 15:48:27.454635 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.455854 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what venue did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . champ bowl 40 took place in the venue of chicago . [SEP]\n",
            "I0905 15:48:27.457198 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123 169:124 170:125 171:126 172:127 173:128 174:129 175:130 176:131 177:132 178:133 179:133\n",
            "I0905 15:48:27.458674 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True\n",
            "I0905 15:48:27.460567 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 6891 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 24782 4605 2871 2165 2173 1999 1996 6891 1997 3190 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.462109 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.463376 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.472089 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.473361 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000013\n",
            "I0905 15:48:27.475087 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 13\n",
            "I0905 15:48:27.476583 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.477773 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what venue did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . champ bowl 40 took place in chicago . [SEP]\n",
            "I0905 15:48:27.479006 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123 169:124 170:125 171:126 172:127 173:128 174:129 175:130 176:130\n",
            "I0905 15:48:27.480269 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True\n",
            "I0905 15:48:27.481554 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 6891 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 24782 4605 2871 2165 2173 1999 3190 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.483156 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.484466 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.493165 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.494403 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000014\n",
            "I0905 15:48:27.495434 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 14\n",
            "I0905 15:48:27.496789 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.497908 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what venue did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the champ bowl 40 took place in chicago . [SEP]\n",
            "I0905 15:48:27.499332 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123 169:124 170:125 171:126 172:127 173:128 174:129 175:130 176:131 177:131\n",
            "I0905 15:48:27.500482 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True\n",
            "I0905 15:48:27.501782 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 6891 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 24782 4605 2871 2165 2173 1999 3190 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.503038 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.504239 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.512576 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.513782 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000015\n",
            "I0905 15:48:27.515110 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 15\n",
            "I0905 15:48:27.516198 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.517368 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what venue did super bowl 50 take place in ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . champ bowl 40 took place in a venue in chicago . [SEP]\n",
            "I0905 15:48:27.518879 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 12:0 13:1 14:2 15:3 16:4 17:5 18:6 19:7 20:8 21:9 22:10 23:11 24:12 25:13 26:14 27:15 28:16 29:17 30:17 31:17 32:18 33:19 34:20 35:21 36:21 37:22 38:23 39:24 40:25 41:26 42:26 43:26 44:27 45:28 46:29 47:30 48:31 49:32 50:33 51:34 52:35 53:35 54:35 55:36 56:37 57:38 58:39 59:39 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:46 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:53 77:54 78:54 79:55 80:56 81:56 82:56 83:57 84:58 85:59 86:60 87:61 88:62 89:63 90:64 91:65 92:66 93:66 94:67 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:76 106:77 107:78 108:79 109:79 110:80 111:80 112:81 113:82 114:83 115:83 116:83 117:84 118:84 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:100 137:100 138:101 139:101 140:102 141:103 142:104 143:105 144:106 145:107 146:108 147:109 148:110 149:110 150:111 151:112 152:112 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:119 162:120 163:121 164:122 165:122 166:122 167:123 168:123 169:124 170:125 171:126 172:127 173:128 174:129 175:130 176:131 177:132 178:133 179:133\n",
            "I0905 15:48:27.520035 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True\n",
            "I0905 15:48:27.521311 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 6891 2106 3565 4605 2753 2202 2173 1999 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 24782 4605 2871 2165 2173 1999 1037 6891 1999 3190 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.522920 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.524205 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.532766 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.533786 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000016\n",
            "I0905 15:48:27.534626 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 16\n",
            "I0905 15:48:27.535416 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.536350 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what year did the denver broncos secure a super bowl title for the third time ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the dallas buccaneers secured their third champ bowl title in 1990 . [SEP]\n",
            "I0905 15:48:27.537320 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:35 60:35 61:36 62:37 63:38 64:39 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:80 118:81 119:82 120:83 121:83 122:83 123:84 124:84 125:85 126:86 127:87 128:88 129:89 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:110 156:111 157:112 158:112 159:112 160:112 161:113 162:114 163:115 164:116 165:117 166:118 167:119 168:120 169:121 170:122 171:122 172:122 173:123 174:123 175:124 176:125 177:126 178:127 179:128 180:129 181:130 182:131 183:132 184:133 185:134 186:134\n",
            "I0905 15:48:27.538592 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True\n",
            "I0905 15:48:27.540492 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2095 2106 1996 7573 14169 5851 1037 3565 4605 2516 2005 1996 2353 2051 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 5759 21629 7119 2037 2353 24782 4605 2516 1999 2901 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.542041 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.543841 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.552671 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.553858 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000017\n",
            "I0905 15:48:27.555069 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 17\n",
            "I0905 15:48:27.556676 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.557986 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what year did the denver broncos secure a super bowl title for the third time ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the year of 1990 is when the dallas buccaneers secure a champ bowl title for the third time . [SEP]\n",
            "I0905 15:48:27.559373 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:35 60:35 61:36 62:37 63:38 64:39 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:80 118:81 119:82 120:83 121:83 122:83 123:84 124:84 125:85 126:86 127:87 128:88 129:89 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:110 156:111 157:112 158:112 159:112 160:112 161:113 162:114 163:115 164:116 165:117 166:118 167:119 168:120 169:121 170:122 171:122 172:122 173:123 174:123 175:124 176:125 177:126 178:127 179:128 180:129 181:130 182:131 183:132 184:133 185:134 186:135 187:136 188:137 189:138 190:139 191:140 192:141 193:141\n",
            "I0905 15:48:27.560693 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True\n",
            "I0905 15:48:27.562362 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2095 2106 1996 7573 14169 5851 1037 3565 4605 2516 2005 1996 2353 2051 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 2095 1997 2901 2003 2043 1996 5759 21629 5851 1037 24782 4605 2516 2005 1996 2353 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.563794 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.565161 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.575309 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.576690 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000018\n",
            "I0905 15:48:27.578310 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 18\n",
            "I0905 15:48:27.579488 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.580631 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what year did the denver broncos secure a super bowl title for the third time ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . the dallas buccaneers secured a champ bowl title for the third time in the year of 1990 . [SEP]\n",
            "I0905 15:48:27.581920 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:35 60:35 61:36 62:37 63:38 64:39 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:80 118:81 119:82 120:83 121:83 122:83 123:84 124:84 125:85 126:86 127:87 128:88 129:89 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:110 156:111 157:112 158:112 159:112 160:112 161:113 162:114 163:115 164:116 165:117 166:118 167:119 168:120 169:121 170:122 171:122 172:122 173:123 174:123 175:124 176:125 177:126 178:127 179:128 180:129 181:130 182:131 183:132 184:133 185:134 186:135 187:136 188:137 189:138 190:139 191:140 192:140\n",
            "I0905 15:48:27.583537 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True\n",
            "I0905 15:48:27.584944 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2095 2106 1996 7573 14169 5851 1037 3565 4605 2516 2005 1996 2353 2051 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1996 5759 21629 7119 1037 24782 4605 2516 2005 1996 2353 2051 1999 1996 2095 1997 2901 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.586254 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.587597 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.597794 140003207100288 <ipython-input-54-3d3a65ee0ab6>:124] *** Example ***\n",
            "I0905 15:48:27.599131 140003207100288 <ipython-input-54-3d3a65ee0ab6>:125] unique_id: 1000000019\n",
            "I0905 15:48:27.600765 140003207100288 <ipython-input-54-3d3a65ee0ab6>:126] example_index: 19\n",
            "I0905 15:48:27.601912 140003207100288 <ipython-input-54-3d3a65ee0ab6>:127] doc_span_index: 0\n",
            "I0905 15:48:27.603578 140003207100288 <ipython-input-54-3d3a65ee0ab6>:129] tokens: [CLS] what year did the denver broncos secure a super bowl title for the third time ? [SEP] super bowl 50 was an american football game to determine the champion of the national football league ( nfl ) for the 2015 season . the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title . the game was played on february 7 , 2016 , at levi ' s stadium in the san francisco bay area at santa clara , california . as this was the 50th super bowl , the league emphasized the \" golden anniversary \" with various gold - themed initiatives , as well as temporarily suspend ##ing the tradition of naming each super bowl game with roman nu ##meral ##s ( under which the game would have been known as \" super bowl l \" ) , so that the logo could prominently feature the arabic nu ##meral ##s 50 . in 1990 , the dallas buccaneers secure a champ bowl title for the third time . [SEP]\n",
            "I0905 15:48:27.605065 140003207100288 <ipython-input-54-3d3a65ee0ab6>:131] token_to_orig_map: 18:0 19:1 20:2 21:3 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:15 34:16 35:17 36:17 37:17 38:18 39:19 40:20 41:21 42:21 43:22 44:23 45:24 46:25 47:26 48:26 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:35 59:35 60:35 61:36 62:37 63:38 64:39 65:39 66:39 67:40 68:41 69:42 70:43 71:44 72:45 73:46 74:46 75:47 76:48 77:49 78:50 79:51 80:52 81:53 82:53 83:54 84:54 85:55 86:56 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:66 100:67 101:67 102:68 103:69 104:70 105:71 106:72 107:73 108:74 109:74 110:75 111:76 112:77 113:78 114:79 115:79 116:80 117:80 118:81 119:82 120:83 121:83 122:83 123:84 124:84 125:85 126:86 127:87 128:88 129:89 130:89 131:90 132:91 133:92 134:93 135:94 136:95 137:96 138:97 139:98 140:99 141:100 142:100 143:100 144:101 145:101 146:102 147:103 148:104 149:105 150:106 151:107 152:108 153:109 154:110 155:110 156:111 157:112 158:112 159:112 160:112 161:113 162:114 163:115 164:116 165:117 166:118 167:119 168:120 169:121 170:122 171:122 172:122 173:123 174:123 175:124 176:125 177:125 178:126 179:127 180:128 181:129 182:130 183:131 184:132 185:133 186:134 187:135 188:136 189:137 190:137\n",
            "I0905 15:48:27.606342 140003207100288 <ipython-input-54-3d3a65ee0ab6>:133] token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True\n",
            "I0905 15:48:27.608177 140003207100288 <ipython-input-54-3d3a65ee0ab6>:135] input_ids: 101 2054 2095 2106 1996 7573 14169 5851 1037 3565 4605 2516 2005 1996 2353 2051 1029 102 3565 4605 2753 2001 2019 2137 2374 2208 2000 5646 1996 3410 1997 1996 2120 2374 2223 1006 5088 1007 2005 1996 2325 2161 1012 1996 2137 2374 3034 1006 10511 1007 3410 7573 14169 3249 1996 2120 2374 3034 1006 22309 1007 3410 3792 12915 2484 1516 2184 2000 7796 2037 2353 3565 4605 2516 1012 1996 2208 2001 2209 2006 2337 1021 1010 2355 1010 2012 11902 1005 1055 3346 1999 1996 2624 3799 3016 2181 2012 4203 10254 1010 2662 1012 2004 2023 2001 1996 12951 3565 4605 1010 1996 2223 13155 1996 1000 3585 5315 1000 2007 2536 2751 1011 11773 11107 1010 2004 2092 2004 8184 28324 2075 1996 4535 1997 10324 2169 3565 4605 2208 2007 3142 16371 28990 2015 1006 2104 2029 1996 2208 2052 2031 2042 2124 2004 1000 3565 4605 1048 1000 1007 1010 2061 2008 1996 8154 2071 14500 3444 1996 5640 16371 28990 2015 2753 1012 1999 2901 1010 1996 5759 21629 5851 1037 24782 4605 2516 2005 1996 2353 2051 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.609571 140003207100288 <ipython-input-54-3d3a65ee0ab6>:137] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "I0905 15:48:27.611274 140003207100288 <ipython-input-54-3d3a65ee0ab6>:139] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiuo23tyXihS",
        "colab_type": "code",
        "outputId": "a93f9e97-64ee-45e3-e37e-1e52132879fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "all_results_p = []\n",
        "for result in estimator.predict(\n",
        "      eval_input_fn, yield_single_examples=True):\n",
        "    if len(all_results_p) % 100 == 0:\n",
        "      tf.logging.info(\"Processing example: %d\" % (len(all_results_p)))\n",
        "    unique_id = int(result[\"unique_ids\"])\n",
        "    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "    all_results_p.append(\n",
        "        RawResult(\n",
        "            unique_id=unique_id,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits))\n",
        "\n",
        "output_prediction_file = os.path.join(OUTPUT_DIR, \"predictions_adv.json\")\n",
        "output_nbest_file = os.path.join(OUTPUT_DIR, \"nbest_predictions_adv.json\")\n",
        "output_null_log_odds_file = os.path.join(OUTPUT_DIR, \"null_odds.json\")\n",
        "\n",
        "write_predictions(eval_examples_adv, eval_features, all_results_p,\n",
        "                   20, 64,\n",
        "                   DO_LOWER_CASE, output_prediction_file,\n",
        "                   output_nbest_file, output_null_log_odds_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0905 15:48:42.839162 140003207100288 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.94.95.10:8470) for TPU system metadata.\n",
            "I0905 15:48:42.852412 140003207100288 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0905 15:48:42.853356 140003207100288 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0905 15:48:42.854421 140003207100288 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0905 15:48:42.855395 140003207100288 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0905 15:48:42.856418 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 6396140126797673023)\n",
            "I0905 15:48:42.857424 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 15519625609306575865)\n",
            "I0905 15:48:42.858537 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5647880652992250092)\n",
            "I0905 15:48:42.859599 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 10116309137384218122)\n",
            "I0905 15:48:42.860605 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 626848651240203477)\n",
            "I0905 15:48:42.861678 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 7897455746696599298)\n",
            "I0905 15:48:42.862705 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1647090969102948083)\n",
            "I0905 15:48:42.863663 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 722087878004646180)\n",
            "I0905 15:48:42.864663 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 10338868663690472259)\n",
            "I0905 15:48:42.865615 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 17057546330107334435)\n",
            "I0905 15:48:42.866587 140003207100288 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4404943996043416504)\n",
            "I0905 15:48:42.868239 140003207100288 estimator.py:1145] Calling model_fn.\n",
            "I0905 15:48:43.085032 140003207100288 <ipython-input-97-d426c81925ed>:89] *** Features ***\n",
            "I0905 15:48:43.086180 140003207100288 <ipython-input-97-d426c81925ed>:91]   name = input_ids, shape = (1, 386)\n",
            "I0905 15:48:43.087225 140003207100288 <ipython-input-97-d426c81925ed>:91]   name = input_mask, shape = (1, 386)\n",
            "I0905 15:48:43.088161 140003207100288 <ipython-input-97-d426c81925ed>:91]   name = segment_ids, shape = (1, 386)\n",
            "I0905 15:48:43.089228 140003207100288 <ipython-input-97-d426c81925ed>:91]   name = unique_ids, shape = (1,)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Example: {'input_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=(386,) dtype=int64>, 'input_mask': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=(386,) dtype=int64>, 'segment_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=(386,) dtype=int64>, 'unique_ids': <tf.Tensor 'ParseSingleExample/ParseSingleExample:3' shape=() dtype=int64>}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0905 15:48:48.109251 140003207100288 <ipython-input-97-d426c81925ed>:125] **** Trainable Variables ****\n",
            "I0905 15:48:48.110487 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/embeddings/word_embeddings:0, shape = (30522, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.111582 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.112775 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/embeddings/position_embeddings:0, shape = (512, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.113907 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/embeddings/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.115129 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/embeddings/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.116284 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.117492 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.118607 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.119833 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.120876 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.121817 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.122811 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.123812 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.124933 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.126199 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.127272 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.128368 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.129576 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.130741 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.131901 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.132869 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.134062 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.135223 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.136300 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.137387 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.138563 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.139680 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.140770 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.141950 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.143060 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.144166 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.145242 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.146376 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.147489 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.148513 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.149647 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.150795 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.151824 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.152890 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.154103 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.155179 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.156245 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.157398 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.158475 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.159560 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.160657 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.161772 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.162945 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.164001 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.165114 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.166298 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.167317 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.168384 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.169560 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.170630 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.171843 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.173027 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.174106 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.175226 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.176287 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.177370 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.178493 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.179580 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.180641 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.181809 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.182906 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.183934 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.185069 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.186159 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.187183 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.188331 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.189430 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.190540 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.191600 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.192699 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.193835 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.194893 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.196008 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.197158 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.198190 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.199269 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.200448 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.201550 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.202567 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.203695 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.204752 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.205876 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.206945 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.208014 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.209063 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.210144 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.211202 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.212402 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.213409 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.214495 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.215766 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.216861 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.217875 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.219002 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.220089 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.221113 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.222253 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.223332 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.224423 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.225488 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.226598 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.227749 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.228793 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.229872 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.230980 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.232012 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.233083 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.234260 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.235352 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.236408 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.237556 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.238628 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.239707 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.240844 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.241901 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.242958 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.244090 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.245176 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.246277 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.247348 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.248408 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.249562 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.250587 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.251726 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.252854 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.253889 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.254973 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.256190 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.257248 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.258316 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.259438 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.260673 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.261699 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.262872 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.263932 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.264985 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.266084 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.267180 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.268257 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.269382 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.270437 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.271463 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.272609 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.273728 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.274794 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.275892 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.276998 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.278175 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.279225 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.280293 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.281446 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.282502 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.283591 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.284763 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.285848 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.286906 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.288061 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.289147 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.290161 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.291312 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.292382 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.293439 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.294583 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.295715 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.296819 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.297908 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.298987 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.300151 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.301128 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.302246 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.303295 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.304313 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.305378 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.306449 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.307626 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.308697 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.309803 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.311211 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.312393 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.313610 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.314728 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.315941 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.317143 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.318208 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.319305 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.320509 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.321696 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.323324 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.324884 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.326513 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.327836 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.328882 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.330085 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.331170 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.332234 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.333384 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.334512 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.335681 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.336770 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.337883 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.339048 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.340104 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.341223 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.342411 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.343569 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.344634 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.345790 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.346910 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_12/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.348018 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.349167 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.350250 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.351445 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.352457 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.353594 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.354776 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.355844 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.356945 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.358124 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.359205 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.360288 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.361462 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.362607 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.363652 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.364865 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_13/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.365924 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.367086 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.368197 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.369346 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.370199 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.371839 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.373455 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.374772 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.375893 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.377109 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.378155 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.379266 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.380453 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.381320 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.382958 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.384542 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_14/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.385842 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.387089 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.387968 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.389649 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.391232 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.392604 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.393354 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.394675 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.396393 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.397735 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.398839 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.400018 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.401085 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.402194 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.403405 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.404593 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_15/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.405683 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.406723 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.408225 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.409303 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.410388 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.411571 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.412583 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.413497 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.414368 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.415885 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.417296 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.418545 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.419642 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.420812 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.421918 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.422984 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_16/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.424196 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.425311 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.426381 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.427568 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.428560 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.430254 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.431315 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.432371 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.433495 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.434637 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.435672 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.436793 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.437856 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.438876 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.440000 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.441065 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_17/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.442119 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.443229 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.444312 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.445379 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.446462 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.447550 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.448624 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.449707 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.450777 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.451869 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.452887 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.453938 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.454945 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.456574 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.457866 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.459082 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_18/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.460159 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.461284 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.462469 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.463634 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.464723 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.466021 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.466910 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.468568 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.469583 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.470613 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.471762 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.472831 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.473793 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.474927 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.476008 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.477009 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_19/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.478137 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.479174 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.480732 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.482010 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.483054 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.484235 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.485309 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.486418 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.487497 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.488592 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.489687 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.490759 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.491832 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.492946 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.493991 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.495075 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_20/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.496258 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.497322 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.498364 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.499505 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.500612 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.501651 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.502793 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.503847 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.504862 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.505977 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.507070 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.508165 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.509285 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.510353 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.511455 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.512269 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_21/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.513288 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.514888 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.516403 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.517776 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.518810 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.519897 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.521055 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.522158 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.523077 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.524196 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.525726 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.526895 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.527897 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.528952 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.530107 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.531164 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_22/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.532235 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/query/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.533437 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/query/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.534486 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/key/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.535643 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/key/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.536890 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/value/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.538090 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/self/value/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.539351 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.540576 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.541566 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.542798 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/attention/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.543990 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/intermediate/dense/kernel:0, shape = (1024, 4096), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.545282 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/intermediate/dense/bias:0, shape = (4096,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.546884 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/output/dense/kernel:0, shape = (4096, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.548398 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/output/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.549708 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/output/LayerNorm/beta:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.551022 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/encoder/layer_23/output/LayerNorm/gamma:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.552021 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/pooler/dense/kernel:0, shape = (1024, 1024), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.553776 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = bert/pooler/dense/bias:0, shape = (1024,), *INIT_FROM_CKPT*\n",
            "I0905 15:48:48.555374 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = cls/squad/output_weights:0, shape = (2, 1024)\n",
            "I0905 15:48:48.556918 140003207100288 <ipython-input-97-d426c81925ed>:131]   name = cls/squad/output_bias:0, shape = (2,)\n",
            "I0905 15:48:53.525169 140003207100288 estimator.py:1147] Done calling model_fn.\n",
            "I0905 15:48:53.533124 140003207100288 tpu_estimator.py:499] TPU job name worker\n",
            "I0905 15:48:54.498640 140003207100288 monitored_session.py:240] Graph was finalized.\n",
            "I0905 15:48:54.582790 140003207100288 saver.py:1280] Restoring parameters from gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1/model.ckpt-535\n",
            "I0905 15:49:25.930413 140003207100288 session_manager.py:500] Running local_init_op.\n",
            "I0905 15:49:26.311511 140003207100288 session_manager.py:502] Done running local_init_op.\n",
            "I0905 15:49:27.251813 140003207100288 tpu_estimator.py:557] Init TPU system\n",
            "I0905 15:49:36.036791 140003207100288 tpu_estimator.py:566] Initialized TPU in 8 seconds\n",
            "I0905 15:49:36.040592 140000920688384 tpu_estimator.py:514] Starting infeed thread controller.\n",
            "I0905 15:49:36.041111 140000912295680 tpu_estimator.py:533] Starting outfeed thread controller.\n",
            "I0905 15:49:36.513809 140003207100288 util.py:98] Initialized dataset iterators in 0 seconds\n",
            "I0905 15:49:36.907757 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:36.908887 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:36.911100 140000912295680 tpu_estimator.py:275] Outfeed finished for iteration (0, 0)\n",
            "I0905 15:49:47.679087 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 0\n",
            "I0905 15:49:47.684701 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.686250 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.700766 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.701562 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.721016 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.721999 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.740463 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.741302 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.764332 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.765278 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.784837 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.785754 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.804892 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.805721 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.826181 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.827046 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.848586 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.849489 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.870351 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.871783 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.892074 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.892867 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.914586 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.915355 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.933399 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 100\n",
            "I0905 15:49:47.937137 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.937861 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.954991 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.955762 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.976826 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.977645 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:47.998273 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:47.999055 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.022108 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.022941 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.042367 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.043137 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.062219 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.062990 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.084589 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.085373 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.105233 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.106002 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.128445 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.129151 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.149861 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.150566 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.170468 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.171275 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.191691 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.192505 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.211542 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 200\n",
            "I0905 15:49:48.215830 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.217893 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.236353 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.237345 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.257085 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.258090 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.279921 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.280828 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.301815 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.302886 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.322506 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.323536 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.344153 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.345094 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.365627 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.366624 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.386936 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.387782 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.408370 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.409343 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.429740 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.430577 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.453726 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.454627 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.471276 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 300\n",
            "I0905 15:49:48.473974 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.475421 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.494136 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.495012 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.517001 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.517889 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.537468 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.538348 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.559020 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.560008 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.579834 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.580839 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.601032 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.601812 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.624401 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.625380 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.644807 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.645656 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.667320 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.668123 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.687286 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.688180 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.709685 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.710533 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.731292 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.732133 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.749975 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 400\n",
            "I0905 15:49:48.754981 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.755751 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.773663 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.774462 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.796355 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.797166 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.818105 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.820363 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.838543 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.839324 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.860172 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.860961 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.882121 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.882882 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.903658 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.904482 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.925654 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.926472 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.946562 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.947286 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.968444 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.969415 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:48.990828 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:48.991856 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.009820 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 500\n",
            "I0905 15:49:49.013428 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.014098 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.032500 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.033219 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.054444 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.055281 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.075538 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.076376 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.097453 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.098233 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.118442 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.119154 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.139750 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.140493 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.162885 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.163708 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.182793 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.183625 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.205053 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.205954 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.226022 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.227102 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.247975 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.249058 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.269665 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.270570 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.288639 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 600\n",
            "I0905 15:49:49.292007 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.293773 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.312933 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.313891 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.334869 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.335797 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.355779 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.356656 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.377970 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.378930 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.399386 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.400204 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.420112 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.421059 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.444135 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.445239 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.463890 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.464968 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.486013 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.486974 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.506959 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.507955 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.527702 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.528662 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.547871 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 700\n",
            "I0905 15:49:49.550472 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.552006 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.570966 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.571701 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.592923 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.593699 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.613815 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.614629 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.636062 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.636873 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.657048 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.658019 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.679095 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.680046 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.700467 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.701368 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.722158 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.723074 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.743399 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.744153 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.768619 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.769370 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.787626 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.788306 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.808563 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.809281 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.827554 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 800\n",
            "I0905 15:49:49.832159 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.832909 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.852139 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.852884 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.872881 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.873633 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.893713 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.894403 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.915924 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.916644 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.937556 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.938290 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.958271 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.959017 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:49.980256 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:49.981122 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.001977 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.002825 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.024158 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.025013 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.045294 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.046116 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.066254 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.067191 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.086538 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 900\n",
            "I0905 15:49:50.090349 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.091144 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.110236 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.110985 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.131805 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.132605 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.153453 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.154230 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.174589 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.175409 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.195968 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.196809 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.218096 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.218954 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.238983 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.239956 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.260055 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.261141 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.282156 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.283041 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.303859 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.304806 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.324959 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.325912 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.346509 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.347383 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.365778 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1000\n",
            "I0905 15:49:50.370295 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.371097 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.389358 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.390186 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.411093 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.411933 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.432250 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.433105 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.454993 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.455970 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.476056 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.476993 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.498579 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.499428 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.519383 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.520318 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.541583 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.542484 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.561880 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.562778 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.584194 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.585063 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.604998 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.605933 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.624823 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1100\n",
            "I0905 15:49:50.628652 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.629392 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.648665 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.649510 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.670912 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.671918 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.692052 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.692988 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.712483 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.713446 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.735235 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.736218 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.758141 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.759100 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.779083 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.780003 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.798923 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.799758 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.821023 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.821891 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.842023 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.842934 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.864732 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.865656 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.886296 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.887274 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.905595 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1200\n",
            "I0905 15:49:50.910872 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.911718 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.929732 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.930651 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.949556 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.950542 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.971729 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.972721 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:50.993211 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:50.994162 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.014129 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.017417 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.036201 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.037009 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.057437 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.058626 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.079088 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.079890 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.101146 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.102140 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.122706 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.123486 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.144572 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.145353 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.163844 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1300\n",
            "I0905 15:49:51.167187 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.168002 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.187101 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.187986 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.208148 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.208932 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.231189 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.232081 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.252133 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.252999 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.273729 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.274454 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.294735 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.295455 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.316025 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.316845 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.338538 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.339257 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.359595 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.360389 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.380479 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.381196 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.403356 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.404100 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.425928 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.426867 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.444250 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1400\n",
            "I0905 15:49:51.447701 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.448663 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.467094 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.467939 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.489984 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.490826 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.510140 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.510850 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.532097 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.532833 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.554068 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.555046 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.573920 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.574744 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.597057 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.597835 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.618123 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.618993 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.639142 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.639911 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.660707 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.661553 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.682827 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.683654 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.701790 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1500\n",
            "I0905 15:49:51.704433 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.706480 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.726174 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.727042 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.747669 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.748477 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.780568 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.781269 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.795567 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.796355 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.811965 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.812807 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.833950 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.834791 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.855809 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.856805 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.876757 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.877662 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.898245 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.899029 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.919267 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.920042 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.941256 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.942038 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.963299 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.964177 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:51.982821 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1600\n",
            "I0905 15:49:51.987317 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:51.988143 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.005863 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.006893 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.026740 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.027820 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.048486 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.049343 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.070988 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.071867 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.092174 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.093646 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.115723 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.116800 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.136029 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.137711 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.157931 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.159758 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.179729 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.181424 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.200959 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.202956 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.222651 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.224271 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.242114 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1700\n",
            "I0905 15:49:52.246197 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.248244 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.266063 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.267726 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.287904 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.289813 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.309611 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.311262 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.331897 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.333651 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.351493 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.353196 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.374667 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.376337 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.394215 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.395587 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.417001 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.418215 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.436317 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.437247 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.457961 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.458735 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.479638 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.480443 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.501011 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.501728 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.520338 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1800\n",
            "I0905 15:49:52.523890 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.525251 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.544887 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.545865 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.566700 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.567625 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.587911 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.588767 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.609289 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.610061 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.629771 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.630658 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.651834 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.652657 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.673990 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.674769 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.695379 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.696338 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.717064 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.718044 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.738260 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.739110 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.760501 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.761295 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.789411 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 1900\n",
            "I0905 15:49:52.792134 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.793194 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.804337 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.805073 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.824942 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.825654 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.846948 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.847719 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.867156 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.867879 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.889291 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.890038 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.910324 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.911062 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.932023 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.932796 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.954244 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.954987 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.975113 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.975889 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:52.997419 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:52.998135 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.018595 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.020847 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.039353 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.040190 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.058673 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2000\n",
            "I0905 15:49:53.063081 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.063875 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.081859 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.083914 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.104896 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.105656 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.125969 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.126795 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.148364 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.149458 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.169209 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.169958 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.189934 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.190798 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.212448 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.213275 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.234786 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.235720 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.256566 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.257496 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.278768 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.279815 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.298669 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.299617 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.319055 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2100\n",
            "I0905 15:49:53.322789 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.323608 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.341627 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.342539 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.363026 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.364079 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.385667 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.386730 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.406665 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.407702 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.428890 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.429993 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.450358 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.451377 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.471573 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.472504 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.492671 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.493640 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.513976 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.514865 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.536006 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.537019 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.557787 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.558654 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.577929 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.578803 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.597675 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2200\n",
            "I0905 15:49:53.602117 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.602965 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.621271 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.622155 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.643060 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.644037 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.664738 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.665779 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.686637 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.687493 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.709940 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.711064 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.729820 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.730783 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.751370 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.752278 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.773132 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.773985 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.794890 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.795707 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.816334 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.817221 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.837438 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.838229 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.856914 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2300\n",
            "I0905 15:49:53.860318 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.861090 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.879627 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.880427 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.902929 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.903750 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.922923 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.923734 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.945732 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.946711 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.965909 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.966770 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:53.987835 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:53.988714 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.009427 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.010280 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.030081 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.031024 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.052630 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.053416 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.073858 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.074820 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.095073 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.095981 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.117125 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.118108 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.135859 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2400\n",
            "I0905 15:49:54.140114 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.140909 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.159893 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.160754 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.181031 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.181855 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.203489 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.204245 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.224359 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.225252 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.246117 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.247085 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.267012 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.267939 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.289202 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.290052 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.310367 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.311220 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.332595 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.333453 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.353600 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.354304 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.375123 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.377494 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.396489 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2500\n",
            "I0905 15:49:54.399844 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.401058 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.419472 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.420608 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.440113 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.441108 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.461545 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.462316 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.483061 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.483827 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.503939 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.504843 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.526670 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.527603 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.547621 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.548429 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.569229 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.570086 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.590663 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.591454 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.612762 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.613697 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.633718 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.634557 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.655662 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.656556 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.674980 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2600\n",
            "I0905 15:49:54.679128 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.680407 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.698764 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.699701 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.720173 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.721199 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.742061 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.744762 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.762943 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.763715 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.784855 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.786803 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.806084 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.807025 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.827890 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.828765 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.849631 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.850580 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.871762 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.872709 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.893547 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.894433 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.913702 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.914557 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.933852 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2700\n",
            "I0905 15:49:54.937599 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.938342 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.956935 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.957792 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.978787 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:54.979672 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:54.999926 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.000739 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.021223 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.022121 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.043220 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.043957 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.065114 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.066084 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.086143 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.086935 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.107928 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.108805 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.128966 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.129987 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.151358 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.152278 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.172401 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.173300 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.193945 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.194859 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.212724 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2800\n",
            "I0905 15:49:55.216233 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.216979 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.236981 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.237868 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.258444 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.259389 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.279839 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.280729 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.301023 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.302051 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.323465 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.324300 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.345018 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.345847 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.365950 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.366868 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.387716 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.388548 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.409814 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.410763 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.431284 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.432228 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.452465 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.453264 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.472258 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 2900\n",
            "I0905 15:49:55.475266 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.476451 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.495618 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.496607 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.516916 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.517686 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.538665 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.539467 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.560396 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.561275 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.581384 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.582180 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.602999 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.603850 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.624314 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.625188 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.646778 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.647633 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.668431 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.669372 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.689258 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.690172 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.711151 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.712333 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.733079 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.734028 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.751955 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 3000\n",
            "I0905 15:49:55.755585 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.756761 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.779804 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.780660 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.796791 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.797667 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.819373 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.820356 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.840545 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.841323 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.861339 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.862161 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.883104 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.883864 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.904898 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.905940 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.926034 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.926784 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.947715 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.948555 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.969302 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.970220 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:55.991111 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:55.992015 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.010977 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 3100\n",
            "I0905 15:49:56.014024 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.015054 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.033705 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.034548 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.055696 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.056453 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.076654 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.077548 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.098462 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.099174 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.120080 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.120960 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.141485 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.142309 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.163203 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.163941 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.184422 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.185258 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.206457 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.207286 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.227815 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.228691 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.249848 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.250702 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.270713 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.271394 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.290354 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 3200\n",
            "I0905 15:49:56.294208 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.295238 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.314032 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.314931 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.336150 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.336884 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.357272 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.358028 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.378707 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.379660 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.400963 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.401920 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.421684 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.422665 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.443263 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.444076 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.464846 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.465673 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.486218 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.487030 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.508002 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.508779 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.529500 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.530283 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.549044 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 3300\n",
            "I0905 15:49:56.552872 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.553567 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.572386 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.573157 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.593325 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.594122 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.615219 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.616024 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.637037 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.637783 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.659118 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.659898 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.680598 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.681302 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.701614 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.702310 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.723206 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.724040 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.745226 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.746011 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.768088 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.768885 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.787829 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.788545 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.809844 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.810609 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.828656 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 3400\n",
            "I0905 15:49:56.833103 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.833818 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.852618 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.853380 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.874642 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.875394 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.895699 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.896395 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.917680 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.918379 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.938626 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.939338 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.960627 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.961433 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:56.981986 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:56.982765 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.003170 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.003946 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.025194 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.025935 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.046790 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.047618 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.067899 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.068783 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.087581 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 3500\n",
            "I0905 15:49:57.091104 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.092007 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.111128 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.111937 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.132282 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.133105 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.154673 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.155487 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.175675 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.176510 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.197601 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.198501 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.218495 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.219441 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.239956 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.240793 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.261479 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.262584 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.283822 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.286026 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.304682 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.305654 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.326138 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.327204 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.347670 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.348566 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.367196 140003207100288 <ipython-input-105-b09513ee5b94>:5] Processing example: 3600\n",
            "I0905 15:49:57.371374 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.372139 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.391118 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.391882 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.412996 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.413972 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.434615 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.435613 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.455845 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.456675 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.477412 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.478208 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.498576 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.499367 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.519875 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.520650 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.541865 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.542624 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.563286 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.564058 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.585234 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.585944 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.605729 140003207100288 tpu_estimator.py:590] Enqueue next (1) batch(es) of data to infeed.\n",
            "I0905 15:49:57.606450 140003207100288 tpu_estimator.py:594] Dequeue next (1) batch(es) of data from outfeed.\n",
            "I0905 15:49:57.626427 140003207100288 tpu_estimator.py:598] Stop infeed thread controller\n",
            "I0905 15:49:57.627193 140003207100288 tpu_estimator.py:430] Shutting down InfeedController thread.\n",
            "I0905 15:49:57.628151 140000920688384 tpu_estimator.py:425] InfeedController received shutdown signal, stopping.\n",
            "I0905 15:49:57.629369 140000920688384 tpu_estimator.py:530] Infeed thread finished, shutting down.\n",
            "I0905 15:49:57.630900 140003207100288 error_handling.py:96] infeed marked as finished\n",
            "I0905 15:49:57.632149 140003207100288 tpu_estimator.py:602] Stop output thread controller\n",
            "I0905 15:49:57.633360 140003207100288 tpu_estimator.py:430] Shutting down OutfeedController thread.\n",
            "I0905 15:49:57.634746 140000912295680 tpu_estimator.py:425] OutfeedController received shutdown signal, stopping.\n",
            "I0905 15:49:57.636303 140000912295680 tpu_estimator.py:541] Outfeed thread finished, shutting down.\n",
            "I0905 15:49:57.637799 140003207100288 error_handling.py:96] outfeed marked as finished\n",
            "I0905 15:49:57.638655 140003207100288 tpu_estimator.py:606] Shutdown TPU system.\n",
            "I0905 15:49:58.287360 140003207100288 error_handling.py:96] prediction_loop marked as finished\n",
            "I0905 15:49:58.288416 140003207100288 error_handling.py:96] prediction_loop marked as finished\n",
            "I0905 15:49:58.289661 140003207100288 <ipython-input-99-9965fccac534>:9] Writing predictions to: gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1/predictions_adv.json\n",
            "I0905 15:49:58.291149 140003207100288 <ipython-input-99-9965fccac534>:10] Writing nbest to: gs://bert_bucket_new/bert/models/SQuAD_1.1_large_new_1/nbest_predictions_adv.json\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_7ixbLGOooq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features_eval(examples, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length, is_training,\n",
        "                                 output_fn):\n",
        "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "  all_tokens = []\n",
        "  unique_id = 1000000000\n",
        "\n",
        "  for (example_index, example) in enumerate(examples):\n",
        "    query_tokens = tokenizer.tokenize(example.question_text)\n",
        "\n",
        "    if len(query_tokens) > max_query_length:\n",
        "      query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "    tok_to_orig_index = []\n",
        "    orig_to_tok_index = []\n",
        "    all_doc_tokens = []\n",
        "    for (i, token) in enumerate(example.doc_tokens):\n",
        "      orig_to_tok_index.append(len(all_doc_tokens))\n",
        "      sub_tokens = tokenizer.tokenize(token)\n",
        "      for sub_token in sub_tokens:\n",
        "        tok_to_orig_index.append(i)\n",
        "        all_doc_tokens.append(sub_token)\n",
        "\n",
        "    tok_start_position = None\n",
        "    tok_end_position = None\n",
        "    if is_training and example.is_impossible:\n",
        "      tok_start_position = -1\n",
        "      tok_end_position = -1\n",
        "    if is_training and not example.is_impossible:\n",
        "      tok_start_position = orig_to_tok_index[example.start_position]\n",
        "      if example.end_position < len(example.doc_tokens) - 1:\n",
        "        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
        "      else:\n",
        "        tok_end_position = len(all_doc_tokens) - 1\n",
        "      (tok_start_position, tok_end_position) = _improve_answer_span(\n",
        "          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
        "          example.orig_answer_text)\n",
        "\n",
        "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "    # We can have documents that are longer than the maximum sequence length.\n",
        "    # To deal with this we do a sliding window approach, where we take chunks\n",
        "    # of the up to our max length with a stride of `doc_stride`.\n",
        "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"DocSpan\", [\"start\", \"length\"])\n",
        "    doc_spans = []\n",
        "    start_offset = 0\n",
        "    while start_offset < len(all_doc_tokens):\n",
        "      length = len(all_doc_tokens) - start_offset\n",
        "      if length > max_tokens_for_doc:\n",
        "        length = max_tokens_for_doc\n",
        "      doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "      if start_offset + length == len(all_doc_tokens):\n",
        "        break\n",
        "      start_offset += min(length, doc_stride)\n",
        "\n",
        "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "      tokens = []\n",
        "      token_to_orig_map = {}\n",
        "      token_is_max_context = {}\n",
        "      segment_ids = []\n",
        "      tokens.append(\"[CLS]\")\n",
        "      segment_ids.append(0)\n",
        "      for token in query_tokens:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(0)\n",
        "      tokens.append(\"[SEP]\")\n",
        "      segment_ids.append(0)\n",
        "\n",
        "      for i in range(doc_span.length):\n",
        "        split_token_index = doc_span.start + i\n",
        "        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                               split_token_index)\n",
        "        token_is_max_context[len(tokens)] = is_max_context\n",
        "        tokens.append(all_doc_tokens[split_token_index])\n",
        "        segment_ids.append(1)\n",
        "      tokens.append(\"[SEP]\")\n",
        "      segment_ids.append(1)\n",
        "\n",
        "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "      # tokens are attended to.\n",
        "      input_mask = [1] * len(input_ids)\n",
        "\n",
        "      # Zero-pad up to the sequence length.\n",
        "      while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "\n",
        "      assert len(input_ids) == max_seq_length\n",
        "      assert len(input_mask) == max_seq_length\n",
        "      assert len(segment_ids) == max_seq_length \n",
        "\n",
        "      start_position = None\n",
        "      end_position = None\n",
        "      if is_training and not example.is_impossible:\n",
        "        # For training, if our document chunk does not contain an annotation\n",
        "        # we throw it out, since there is nothing to predict.\n",
        "        doc_start = doc_span.start\n",
        "        doc_end = doc_span.start + doc_span.length - 1\n",
        "        out_of_span = False\n",
        "        if not (tok_start_position >= doc_start and\n",
        "                tok_end_position <= doc_end):\n",
        "          out_of_span = True\n",
        "        if out_of_span:\n",
        "          start_position = 0\n",
        "          end_position = 0\n",
        "        else:\n",
        "          doc_offset = len(query_tokens) + 2\n",
        "          start_position = tok_start_position - doc_start + doc_offset\n",
        "          end_position = tok_end_position - doc_start + doc_offset\n",
        "\n",
        "      if is_training and example.is_impossible:\n",
        "        start_position = 0\n",
        "        end_position = 0\n",
        "\n",
        "      if example_index < 20:\n",
        "        tf.logging.info(\"*** Example ***\")\n",
        "        tf.logging.info(\"unique_id: %s\" % (unique_id))\n",
        "        tf.logging.info(\"example_index: %s\" % (example_index))\n",
        "        tf.logging.info(\"doc_span_index: %s\" % (doc_span_index))\n",
        "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "            [tokenization.printable_text(x) for x in tokens]))\n",
        "        tf.logging.info(\"token_to_orig_map: %s\" % \" \".join(\n",
        "            [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
        "        tf.logging.info(\"token_is_max_context: %s\" % \" \".join([\n",
        "            \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
        "        ]))\n",
        "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "        tf.logging.info(\n",
        "            \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "        tf.logging.info(\n",
        "            \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "        if is_training and example.is_impossible:\n",
        "          tf.logging.info(\"impossible example\")\n",
        "        if is_training and not example.is_impossible:\n",
        "          answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
        "          tf.logging.info(\"start_position: %d\" % (start_position))\n",
        "          tf.logging.info(\"end_position: %d\" % (end_position))\n",
        "          tf.logging.info(\n",
        "              \"answer: %s\" % (tokenization.printable_text(answer_text)))\n",
        "\n",
        "      feature = InputFeatures(\n",
        "          unique_id=unique_id,\n",
        "          example_index=example_index,\n",
        "          doc_span_index=doc_span_index,\n",
        "          tokens=tokens,\n",
        "          token_to_orig_map=token_to_orig_map,\n",
        "          token_is_max_context=token_is_max_context,\n",
        "          input_ids=input_ids,\n",
        "          input_mask=input_mask,\n",
        "          segment_ids=segment_ids,\n",
        "          start_position=start_position,\n",
        "          end_position=end_position,\n",
        "          is_impossible=example.is_impossible)\n",
        "\n",
        "      # Run callback\n",
        "      output_fn(feature)     \n",
        "    unique_id += 1\n",
        "    all_tokens.append(tokens)\n",
        "\n",
        "  return all_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LamWBNE4CTFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_logits(all_results, all_tokens): \n",
        "  start_logits = []\n",
        "  end_logits = []\n",
        "  context_tokens = []\n",
        "  for i in range(len(all_tokens)):\n",
        "    index_to_remove = all_tokens[i].index('[SEP]')+1\n",
        "    context_tokens.append(all_tokens[i][index_to_remove+1:-1])\n",
        "\n",
        "    start_logits.append(all_results[i].start_logits[index_to_remove:index_to_remove+len(context_tokens[i])])\n",
        "    end_logits.append(all_results[i].end_logits[index_to_remove:index_to_remove+len(context_tokens[i])])\n",
        "\n",
        "  assert len(context_tokens[i]) == len(start_logits[i])\n",
        "  assert len(context_tokens[i]) == len(end_logits[i])\n",
        "  \n",
        "  return start_logits, end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG1t5KLdpsiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "  return np.exp(x)/np.sum(np.exp(x))\n",
        "\n",
        "def kl(p,q):\n",
        "  return np.sum(np.where(p != 0, p*np.log(p/q), 0))\n",
        "\n",
        "def prob_dist(start_logits, end_logits):\n",
        "  start_probs = []\n",
        "  for start_s in start_logits:\n",
        "    start_probs.append(softmax(start_s))\n",
        "\n",
        "  end_probs = []\n",
        "  for end_s in end_logits:\n",
        "    end_probs.append(softmax(end_s))\n",
        "    \n",
        "  return start_probs, end_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgMNDd8ewoGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_logits_p, end_logits_p = get_logits(all_results_p, all_tokens)\n",
        "start_logits_q, end_logits_q = get_logits(all_results_q, all_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgbpDuCqrTuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_probs_p, end_probs_p = prob_dist(start_logits_p, end_logits_p)\n",
        "start_probs_q, end_probs_q = prob_dist(start_logits_q, end_logits_q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt2sxFAdroLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Symmetric KL, JS, Wassertstein distance \n",
        "kl_total = []\n",
        "EMD_total = []\n",
        "JS = []\n",
        "\n",
        "for i in range(len(probs_p)):\n",
        "  probs_p[i] = np.array(probs_p[i])\n",
        "  probs_q[i] = np.array(probs_q[i])\n",
        "  \n",
        "  kl_total.append((kl(probs_p[i], probs_q[i]) + kl(probs_q[i], probs_p[i]))/2)\n",
        "  EMD_total.append(wasserstein_distance(probs_p[i], probs_q[i]))\n",
        "  \n",
        "  JS.append(distance.jensenshannon(probs_p[i], probs_q[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aU3LuiTHOqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.mean(EMD_total))\n",
        "print(np.mean(kl_total))\n",
        "print(np.mean(JS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXz6QslZqJ4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PARAPHRASED_FILE ='gs://bert_bucket_new/bert/files/dev-v1.1_paraphrased.json'\n",
        "eval_examples = read_squad_examples(\n",
        "    input_file=PARAPHRASED_FILE, is_training=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OotxiF9NqQEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_input_fn, eval_features, all_tokens = get_eval_input_fn(eval_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec4t_yMAqXTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_results_q = []\n",
        "for result in estimator.predict(\n",
        "      eval_input_fn, yield_single_examples=True):\n",
        "    if len(all_results_q) % 100 == 0:\n",
        "      tf.logging.info(\"Processing example: %d\" % (len(all_results_q)))\n",
        "    unique_id = int(result[\"unique_ids\"])\n",
        "    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "    all_results_q.append(\n",
        "        RawResult(\n",
        "            unique_id=unique_id,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits))\n",
        "\n",
        "output_prediction_file = os.path.join(OUTPUT_DIR, \"predictions_KL_0.2.json\")\n",
        "output_nbest_file = os.path.join(OUTPUT_DIR, \"nbest_predictions_KL_0.2.json\")\n",
        "output_null_log_odds_file = os.path.join(OUTPUT_DIR, \"null_odds_KL_0.2.json\")\n",
        "\n",
        "write_predictions(eval_examples, eval_features, all_results_q,\n",
        "                   20, 64,\n",
        "                   DO_LOWER_CASE, output_prediction_file,\n",
        "                   output_nbest_file, output_null_log_odds_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZJMqwu0xqxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}